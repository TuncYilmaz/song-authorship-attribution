{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we'll prepare our dataset for the character model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by using some pickle helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def writePickle(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "def readPickle(fname):\n",
    "    filename = \"pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "def readPicklePast(fname):\n",
    "    filename = \"../pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also load the complete sub_dataset from the csv file, and get all examples with their corresponding artist labels (categorical) and genre labels (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    data = pd.read_csv('sub_dataset.csv', header=None)\n",
    "    data = data.dropna()\n",
    "\n",
    "    x = data[2]\n",
    "    x = np.array(x)\n",
    "\n",
    "    y_artist = data[0] - 1\n",
    "    y_artist = to_categorical(y_artist)\n",
    "    \n",
    "    y_genre = data[1] - 1\n",
    "    y_genre = to_categorical(y_genre)\n",
    "    \n",
    "    return (x, y_artist, y_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_artist, y_genre = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also load the genre and artist id mapping dictionaries\n",
    "artist2id = readPickle(\"artist2id\")\n",
    "id2artist = readPickle(\"id2artist\")\n",
    "genre2id = readPickle(\"genre2id\")\n",
    "id2genre = readPickle(\"id2genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Someone wrote just yesterday\n",
      "\"Love's in need of love today\"\n",
      "Will there be none tomorrow\n",
      "\n",
      "Faded feelings, jaded news\n",
      "Where's the tender hearts\n",
      "That love romance, they\n",
      "Seem so few\n",
      "\n",
      "So, I write these words to say\n",
      "Take heart have faith\n",
      "In love's tomorrow\n",
      "Where our precious dreams\n",
      "Will all come true\n",
      "\n",
      "Heaven knows what I've\n",
      "Been through\n",
      "Searching for a heart that's true\n",
      "Is there one in a million?\n",
      "\n",
      "Though I've often failed the test\n",
      "Everyone must journey till\n",
      "That questioning is through\n",
      "\n",
      "Shine a star for me to see what\n",
      "Children see on Christmas morning\n",
      "Every hope and dream\n",
      "To be renewed\n",
      "\n",
      "Heaven only knows how I hope\n",
      "That chance of love is higher\n",
      "Than one in a million\n",
      "\n",
      "I'm a dreamer I confess\n",
      "But my dreams are only of\n",
      "The best that we can do\n",
      "\n",
      "So, I'll keep on counting sheep\n",
      "And will not sleep until the dawning\n",
      "\n",
      "Help me break the news, eternal\n",
      "Love just beat the blues\n",
      "\n",
      "Save me from no love\n",
      "(You oughta save me)\n",
      "Save me from no love\n",
      "Say sweet love has come\n",
      "And is not gone,\n",
      "There is all, that I want,\n",
      "This morning\n",
      "\n",
      "Save me from no love\n",
      "(You oughta save me)\n",
      "Save me from no love\n",
      "Say sweet love has come\n",
      "And will live on,\n",
      "Never gone, ever on,\n",
      "(Strong), eternally (forever)\n",
      "\n",
      "Now I've done the best I can, girl\n",
      "I need a helping hand\n",
      "Are you one in a million?\n",
      "\n",
      "People leave without good-byes\n",
      "Stay with me, for through your\n",
      "Eyes I see another truth\n",
      "\n",
      "You're the star and now I see what\n",
      "Children see on Christmas morning\n",
      "\n",
      "Stay and see it through the gift\n",
      "Of love returned to you\n",
      "\n",
      "\n",
      "I'm searchin' for love, baby,\n",
      "Been lookin' today\n",
      "Somewhere there's a love, baby,\n",
      "Do you know the way?\n",
      "\n",
      "\n",
      "Save me, save me\n",
      "You've got to save me\n",
      "Save me, save me\n",
      "Baby, you can save me\n",
      "Save me, save me\n",
      "You've got to save me\n",
      "From no love \n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Al Jarreau \n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] Jazz\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print(x[100],'\\n', y_artist[100], id2artist[1 + np.argmax(y_artist[100], axis=-1)],'\\n', \\\n",
    "      y_genre[100], id2genre[1 + np.argmax(y_genre[100], axis=-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this preprocessing document, we'll form character embeddings that are derived from GloVe word embeddings.\n",
    "Therefore we need to get the glove data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-26 18:25:01--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-08-26 18:25:02--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-08-26 18:25:03--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.32MB/s    in 6m 43s  \n",
      "\n",
      "2020-08-26 18:31:47 (2.04 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the file\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
    "zip_ref.extractall('glove')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings and write them in a dictionary\n",
    "import os\n",
    "import io\n",
    "embeddings_index = {}\n",
    "with io.open('glove/glove.6B.300d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:],dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('so', array([-2.4561e-01,  6.8010e-02,  1.8254e-01, -2.9551e-01,  1.8007e-02,\n",
       "          1.6811e-01,  1.9109e-01,  6.3650e-02,  2.1243e-01, -2.1165e+00,\n",
       "          3.7864e-01, -1.0974e-01, -2.0340e-01,  8.5525e-02, -3.2146e-01,\n",
       "          4.8442e-02, -2.7847e-01, -2.7883e-01,  1.2850e-01, -1.0495e-01,\n",
       "          1.2134e-01,  4.5983e-01,  1.2976e-01,  3.0153e-02, -3.3020e-01,\n",
       "          2.3499e-01,  1.5116e-01, -2.5010e-01, -1.2135e-01,  4.0642e-01,\n",
       "          5.2263e-02,  3.2016e-01, -4.2218e-01, -1.5186e-01, -1.0565e+00,\n",
       "          3.0503e-01, -1.1422e-03, -9.0414e-02, -7.5197e-02, -7.9242e-02,\n",
       "         -3.4756e-02, -1.4131e-01, -1.9038e-01, -7.7423e-02,  4.8919e-02,\n",
       "          2.4843e-01,  1.9439e-01, -1.0053e-01,  1.1012e-01,  3.0298e-01,\n",
       "          2.9027e-01, -2.7237e-01,  8.6791e-02, -1.4677e-01, -3.8868e-01,\n",
       "          2.1407e-01,  1.4234e-02,  5.5393e-02,  1.7376e-01,  6.5738e-02,\n",
       "          4.0475e-01, -2.9207e-02,  1.8231e-01,  4.0401e-01, -1.5855e-01,\n",
       "         -2.3270e-01,  2.3904e-01,  1.8642e-01,  8.0885e-02,  2.3269e-02,\n",
       "          2.3239e-02,  5.1655e-02,  9.2405e-02,  6.3031e-01,  2.5124e-01,\n",
       "         -2.0871e-01,  2.7705e-01,  2.4483e-01, -4.0881e-01, -9.9428e-02,\n",
       "         -2.3452e-01, -6.5307e-02,  4.6555e-01, -1.2339e-01,  6.7672e-02,\n",
       "          2.6328e-01, -1.9264e-01,  3.6981e-01,  3.2269e-02,  2.1389e-01,\n",
       "         -3.3918e-01,  3.3028e-01, -3.8563e-01, -2.9815e-01, -1.5889e-01,\n",
       "         -5.4483e-02, -3.1472e-01,  6.1231e-02,  7.5439e-02, -2.1717e-01,\n",
       "          5.7188e-02, -1.1344e-01, -1.4499e-02, -8.8997e-02, -9.3028e-02,\n",
       "          5.5303e-02,  2.3446e-01,  1.6238e-01, -2.5152e-01,  1.7325e-01,\n",
       "         -2.5444e-01, -1.8785e-01, -1.9325e-01, -1.7256e-01, -1.1149e-01,\n",
       "          4.3777e-02, -2.2146e-01, -1.2673e-01,  1.5096e-01, -2.9503e-01,\n",
       "          4.0220e-02, -2.3690e-01,  1.2001e-01,  3.9648e-01,  2.5297e-01,\n",
       "          7.9608e-02, -1.4299e-01,  2.7292e-01, -1.6782e-01,  1.5189e-01,\n",
       "          2.1302e-01,  2.1094e-01,  2.1875e-01,  1.4928e-01, -3.8411e-01,\n",
       "         -9.1750e-02,  1.7373e-01, -1.6475e-01,  5.7943e-02,  1.0124e-01,\n",
       "         -8.0045e-02,  3.1523e-01, -1.4418e-02,  3.4638e-01, -6.6438e-01,\n",
       "         -3.1016e-01,  2.4332e-01,  5.9218e-02, -1.5125e-02, -7.9031e-02,\n",
       "         -1.4769e-01, -2.6273e-01,  1.2309e-02, -7.4354e-02,  3.4916e-01,\n",
       "          1.4601e-02,  4.1956e-02, -2.6348e-01,  9.2800e-02, -1.7278e-01,\n",
       "         -3.7439e-02, -4.5319e-01,  8.4474e-02,  8.8164e-02,  2.1473e-01,\n",
       "         -6.6083e-02,  4.5427e-03, -4.8284e-02,  8.9250e-02,  6.2916e-02,\n",
       "          8.8596e-02,  3.0438e-01, -6.8658e-01,  2.9040e-02, -7.5351e-02,\n",
       "         -2.2897e-01, -1.2309e-01,  1.4502e-01,  9.4642e-02,  3.3689e-01,\n",
       "          9.4013e-02,  4.6941e-02, -4.2678e-02, -2.3334e-01,  2.3579e-01,\n",
       "         -1.8884e-01,  8.1342e-02, -2.8924e-02,  2.6360e-01,  1.7096e-01,\n",
       "         -5.4196e-02, -1.6244e-01,  1.6515e-01, -1.3793e-01,  1.4287e-01,\n",
       "         -3.0876e-01, -2.8393e-01,  1.6798e-01,  6.7876e-02, -3.9410e-01,\n",
       "          1.0744e+00, -1.5689e-01,  1.2324e-01,  4.9022e-02,  1.5685e-01,\n",
       "         -8.8788e-02,  1.1734e-01,  1.0831e-01,  2.2051e-01, -2.5443e-01,\n",
       "         -2.4655e-01, -1.1989e-01,  1.8085e-01,  1.2390e-01, -7.2709e-02,\n",
       "          1.1970e-01,  1.3120e-01,  1.5384e-01,  1.3839e-01,  6.3832e-02,\n",
       "          3.1536e-01,  9.5940e-02, -6.5503e-02,  2.9662e-02,  9.7104e-02,\n",
       "          1.2741e-01,  1.6053e-01, -1.1504e-01, -3.1150e-01, -9.2226e-02,\n",
       "          5.6706e-02, -9.1555e-02,  1.0222e-01, -2.1327e-01, -2.9250e-01,\n",
       "          1.3157e-01,  2.1655e-02,  6.7486e-02, -2.0800e-01,  1.0391e-01,\n",
       "          3.2409e-01,  8.1532e-02,  3.3266e-01,  3.4256e-01, -7.8900e-01,\n",
       "         -1.0597e-01,  1.4942e-01,  1.0774e-01, -4.9388e-02,  7.7332e-02,\n",
       "          1.5184e-01, -3.9061e-01, -1.0818e-01,  9.7691e-02,  6.4866e-01,\n",
       "          3.0816e-02, -7.3734e-02, -2.8047e-01,  6.6118e-02,  4.8955e-02,\n",
       "         -2.0672e-01, -2.6950e-01, -3.6829e-02,  8.0812e-02,  3.8849e-02,\n",
       "          1.6104e-01, -1.1471e-01, -7.3495e-02,  2.9655e-01,  1.8030e-01,\n",
       "          8.6003e-02,  2.5727e-02,  8.0528e-02,  2.1477e-01,  4.6840e-02,\n",
       "          3.0134e-01, -2.1721e+00, -1.3761e-01,  1.8022e-01,  1.8490e-01,\n",
       "         -2.0713e-01,  1.2292e-01,  2.0426e-01, -8.1932e-02, -2.3247e-01,\n",
       "          2.3988e-01,  1.5321e-01,  1.8745e-01, -4.0118e-01, -2.7027e-01,\n",
       "          9.5983e-02, -3.4739e-01,  3.2640e-02,  4.7579e-02,  2.2242e-01,\n",
       "         -2.3140e-01,  1.1431e-01, -6.6914e-01, -3.4753e-02,  9.7869e-02],\n",
       "        dtype=float32)),\n",
       " ('them',\n",
       "  array([-2.0588e-01,  1.9257e-01, -1.1827e-01, -1.5712e-01, -3.8016e-01,\n",
       "          5.9840e-02, -3.2788e-02,  1.7465e-01,  4.0397e-01, -1.8908e+00,\n",
       "          1.0638e-01, -3.6476e-01,  5.4180e-02, -2.7433e-02, -1.8836e-01,\n",
       "          1.3550e-01, -2.3994e-01, -7.1888e-02,  3.6354e-01, -1.2846e-01,\n",
       "         -1.4596e-03,  6.1712e-02,  5.8014e-01, -3.8355e-02, -2.7338e-01,\n",
       "         -2.8991e-01, -3.3286e-01,  5.5739e-01,  1.4524e-01,  1.5591e-02,\n",
       "         -1.8613e-01,  1.0440e-01,  2.4976e-02, -4.1022e-01, -1.0108e+00,\n",
       "         -7.2071e-02, -1.6702e-02,  2.8830e-01,  1.2603e-01, -4.4816e-01,\n",
       "          4.5100e-01, -2.2655e-01, -3.2827e-01, -3.2651e-02,  7.3625e-02,\n",
       "          4.5860e-01, -2.1645e-02,  2.2891e-01, -2.5990e-01, -1.7484e-01,\n",
       "          2.4557e-01, -2.7777e-01, -1.5828e-01, -3.8369e-01, -1.0561e-01,\n",
       "          1.5613e-01, -2.4917e-01,  1.4219e-01, -1.8627e-01,  1.3364e-01,\n",
       "         -5.2249e-02,  2.8225e-02,  3.1456e-01,  4.6253e-01,  1.5884e-01,\n",
       "         -3.1894e-01,  1.7882e-01,  1.8628e-02, -6.7114e-02,  2.4124e-01,\n",
       "         -4.8570e-02, -3.1391e-01, -3.0467e-01,  2.6682e-01,  1.5980e-01,\n",
       "         -1.0008e-01,  2.5086e-01,  3.2253e-01, -5.0946e-03, -4.6566e-01,\n",
       "         -2.0349e-02, -1.6525e-01, -1.2489e-02,  1.5006e-01, -1.9005e-01,\n",
       "         -9.6128e-03, -4.3674e-02,  1.1080e-01,  1.2821e-01,  2.0236e-01,\n",
       "         -3.6321e-01,  4.4662e-01, -1.1646e-01,  2.6071e-01,  9.6186e-02,\n",
       "         -4.8850e-02, -4.1692e-01, -3.2092e-01,  1.7508e-01, -1.1090e-01,\n",
       "          2.5188e-01, -1.6408e-01, -7.0143e-02, -3.0435e-01,  6.3630e-02,\n",
       "          3.5219e-02,  1.2051e-01, -5.7574e-02, -2.2574e-01,  1.4639e-01,\n",
       "          1.6670e-01, -1.5536e-02,  2.0760e-02, -3.2210e-01, -2.8575e-02,\n",
       "          3.9123e-01, -6.1909e-01,  2.2785e-01,  3.3463e-01, -3.8201e-01,\n",
       "         -3.7438e-01, -1.6817e-01,  1.5756e-01,  3.1505e-01, -1.9148e-01,\n",
       "         -1.2333e-01,  1.9935e-03, -9.0759e-02,  1.3416e-01, -2.0724e-01,\n",
       "          2.5103e-01,  2.6899e-02,  2.9905e-01,  4.6795e-01, -3.1818e-01,\n",
       "         -1.3788e-02,  2.2120e-01, -2.6518e-01, -7.1694e-02,  3.0958e-01,\n",
       "          3.1732e-02,  7.2547e-02, -3.3979e-01,  4.9371e-01, -5.6061e-01,\n",
       "         -1.7703e-01,  1.7065e-01, -1.3747e-01,  2.8845e-01, -2.9284e-01,\n",
       "         -1.9340e-01,  2.6362e-02, -3.7463e-02, -1.8939e-01,  2.5657e-01,\n",
       "          2.0978e-01, -2.3359e-01, -2.9879e-01,  4.2495e-02, -1.1823e-01,\n",
       "          7.7192e-02, -3.3069e-01,  2.7392e-01, -1.1607e-01,  4.5931e-03,\n",
       "          1.7704e-01,  3.6675e-01,  2.0495e-01,  5.8702e-02, -2.3660e-02,\n",
       "          6.6573e-02, -2.5620e-01, -4.8375e-01,  2.4518e-01,  2.3128e-01,\n",
       "         -1.2906e-01, -3.2970e-01,  4.2850e-01, -2.3110e-01,  4.0057e-01,\n",
       "          4.6974e-01, -2.5586e-01,  3.0898e-01, -1.4363e-01, -1.8792e-01,\n",
       "          6.8157e-02, -1.5757e-01,  1.4562e-01,  5.5727e-01, -1.1218e-02,\n",
       "         -1.2420e-02,  1.7634e-01,  7.1434e-02, -1.0401e-01,  2.8935e-01,\n",
       "         -2.1226e-01, -2.5880e-01,  9.3032e-02,  4.5406e-01, -1.8511e-01,\n",
       "          6.4489e-01, -3.2990e-01, -1.6341e-01,  2.0429e-01,  2.1976e-01,\n",
       "         -5.8240e-02,  3.1322e-01,  1.4362e-01, -5.8819e-02, -5.2423e-01,\n",
       "         -3.1484e-01, -8.3798e-02,  3.2035e-01,  1.2847e-02,  2.9856e-01,\n",
       "         -4.1638e-02,  2.5574e-01,  4.9468e-01, -2.5885e-01,  2.0290e-01,\n",
       "          2.6486e-01, -1.4175e-01, -9.8492e-03, -4.5118e-02, -2.7711e-01,\n",
       "         -2.1211e-01,  5.4878e-01,  3.2803e-01, -1.2014e-01, -2.0164e-01,\n",
       "          1.6260e-01, -3.7302e-03, -2.8852e-01, -2.7764e-01,  6.0362e-02,\n",
       "         -2.3966e-01, -2.6602e-01,  1.5617e-01, -1.4668e-01,  1.1793e-01,\n",
       "         -7.7807e-02,  1.8101e-01,  4.3906e-01,  3.6572e-01, -5.4657e-01,\n",
       "         -4.3198e-02,  2.4796e-01,  1.6400e-01, -2.6369e-01, -2.2494e-01,\n",
       "         -8.1996e-02, -2.4720e-01, -1.8063e-01, -2.2278e-02,  8.4354e-01,\n",
       "          1.0819e-01, -1.5677e-01, -1.2637e-01, -2.5005e-02, -2.3598e-02,\n",
       "          1.4412e-01, -6.4454e-01,  2.3286e-01,  2.1366e-02,  1.7700e-01,\n",
       "          1.4888e-01, -3.3053e-01, -1.8022e-01, -3.1249e-01, -1.6835e-01,\n",
       "          1.9844e-01,  2.1983e-02,  1.6749e-01,  6.0130e-01,  2.0274e-01,\n",
       "          1.5445e-03, -2.7504e+00, -1.9060e-01,  1.5862e-01,  1.0331e-01,\n",
       "         -2.7867e-01,  4.4422e-01,  9.2754e-02,  2.3597e-01, -7.0352e-02,\n",
       "          4.3841e-01,  1.1230e-01,  3.5573e-01,  1.8423e-01, -1.5848e-01,\n",
       "         -5.7513e-02, -6.5060e-01,  2.8065e-02, -1.3532e-01,  1.1599e-01,\n",
       "         -2.4449e-01,  4.8469e-02, -3.2868e-01, -1.6960e-01, -1.2965e-01],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see an example\n",
    "list(embeddings_index.items())[100:102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'd like to see the complete set of characters used in the whole dataset. We'll also check whether each of these characters are/are not present in the Glove embedding index matrix. If a character is in our embedding index matrix, that character will be added to our ultimate alphabet. Otherwise it will be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'L', 'K', 'D', 'U', '/', '8', 'â', 'צ', 'ῖ', 'Þ', '\\n', 'ù', 'ב', 'ῦ', 'Β', 'ï', '&', 'ã', 'Φ', 'Z', '抮', 'る', 'ŋ', 'ᾷ', 'Ρ', 'w', 'h', 'ל', '¿', 'ῶ', '|', '1', 'Ε', 'ὺ', 'u', 'm', '抎', 'ἴ', 'f', ':', 'W', 'р', 'C', '\"', 'я', 'θ', 'Ν', 'ο', 'נ', 'ê', 'σ', 'ἄ', 'д', '}', '`', 'γ', 'N', 'Η', 'j', '愛', 'с', 'έ', 'Λ', 'π', 'ζ', 'Ὄ', 'T', 'ἤ', 'H', '©', '_', 'ν', 'τ', 'β', 'º', 'ç', 'ו', 'æ', '-', '“', '扢', 'ю', 'м', '(', 'п', 'ὸ', 'Ἀ', 'k', 'l', '@', 'p', 'Κ', 'ח', 'ἐ', 'ч', 'ף', 'ü', 'ð', 'Ψ', 'ί', 'П', 'Γ', 'а', 'ύ', 'ע', 'г', '2', '\\\\', '•', 'Ἔ', 'ή', 'd', 'á', 'ά', 'Э', 'Q', '3', 'M', 'У', 'ó', 'Σ', 'e', 'ā', 'λ', 'ω', 'O', 'ὁ', 'ם', 'z', 'ך', 'Ó', 'þ', 'g', '0', 'E', '¯', 'ס', 'ь', 'Θ', 'Ὁ', '*', 'G', 'ă', 'し', 'ן', 'н', 'л', 'ξ', '<', 'ä', '—', 'Ο', '抰', 'ε', 'κ', 'э', '¡', 'Ý', '{', 'é', '扴', 'φ', 'í', 'ú', 'I', 'à', '扵', 'Α', 'ש', 'α', 'ñ', 'ἀ', 'ὐ', 'η', 'ἱ', '?', '[', 'o', ')', 'c', 'ý', 'ö', '抯', 'ò', 'v', 'ד', 'ὰ', 'ἰ', 'ι', 'ì', 'A', '抳', 'î', 'て', 'q', 'י', '%', 'μ', ' ', 'ὶ', 'x', '扡', 'ώ', '+', '!', '′', 'ז', 'Â', 'è', 'Υ', 'и', 'χ', '”', 'Ζ', 'δ', 'כ', 'Δ', '~', 'Ι', 'ג', 'ς', 'a', 'i', '³', 't', 'ш', 'Ü', 'V', 'y', 'n', 'к', '°', 'R', ',', 'Τ', 'å', 'й', 'F', '#', 'Μ', '5', 'Ô', 'ﬁ', '抣', '9', 'œ', 'ы', '…', '\\t', 'Ω', 'ὲ', 'ת', 'Ã', 'Χ', 'r', '$', '®', '=', 'א', 'J', 'ר', 'Ἴ', 'B', '\\u2028', 'ה', 'е', 'Н', 'Y', '抦', 'ὄ', 'т', 'ρ', '\\x7f', '>', ']', 'Π', '‘', 'ὀ', 'ὴ', 'Ἐ', 's', 'ό', 'š', 'υ', 'b', '’', 'ὅ', 'É', '4', \"'\", 'û', '6', '¦', '＇', 'מ', '^', '–', '7', 'P', ';', 'X', 'S', 'Ὅ']\n"
     ]
    }
   ],
   "source": [
    "characters = []\n",
    "for song in x:\n",
    "    for char in song:\n",
    "        characters.append(char)\n",
    "\n",
    "        \n",
    "alphabet = list(set(characters))\n",
    "\n",
    "vocab_size = len(alphabet)\n",
    "\n",
    "# create dictionaries for letter indexing\n",
    "vocab = {}\n",
    "reverse_vocab = {}\n",
    "for i, letter in enumerate(alphabet):\n",
    "    vocab[letter] = i\n",
    "    reverse_vocab[i] = letter\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '/', '8', 'â', 'ב', 'ï', '&', 'ã', 'ŋ', 'w', 'h', '¿', '|', '1', 'u', 'm', 'f', ':', '\"', 'я', 'θ', 'ο', 'ê', 'σ', 'д', '}', '`', 'γ', 'j', 'с', 'π', 'ζ', '_', 'ν', 'τ', 'β', 'ç', 'ו', 'æ', '-', '“', 'ю', 'м', '(', 'п', 'k', 'l', '@', 'p', 'ч', 'ü', 'ð', 'а', 'г', '2', '\\\\', 'd', 'á', '3', 'ó', 'e', 'ā', 'λ', 'ω', 'ὁ', 'z', 'þ', 'g', '0', 'ס', 'ь', '*', 'ă', 'н', 'л', 'ξ', '<', 'ä', '—', 'ε', 'κ', 'э', '¡', '{', 'é', 'φ', 'í', 'ú', 'à', 'ש', 'α', 'ñ', 'η', '?', '[', 'o', ')', 'c', 'ý', 'ö', 'ò', 'v', 'ד', 'ι', 'î', 'q', 'י', '%', 'μ', 'x', '+', '!', 'è', 'и', 'χ', '”', 'δ', '~', 'a', 'i', 't', 'ш', 'y', 'n', 'к', ',', 'å', '#', '5', '9', 'œ', '…', 'r', '$', '=', 'א', 'ר', 'ה', 'е', 'т', 'ρ', '>', ']', '‘', 's', 'š', 'υ', 'b', '’', '4', \"'\", 'û', '6', '^', '–', '7', ';'] 157\n"
     ]
    }
   ],
   "source": [
    "ultimate_alphabet = list()\n",
    "all_embedding_keys = list(embeddings_index.keys())\n",
    "for char in alphabet:\n",
    "    if char in all_embedding_keys:\n",
    "        ultimate_alphabet.append(char)\n",
    "print(ultimate_alphabet, len(ultimate_alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have to consider several important things: <br>\n",
    "1- All our characters with vectors are lowercase. Therefore we need our dataset converted to lowercase.<br>\n",
    "2- We need another vector for the space and linebrake characters. The choice of having a linebrake might be crucial.<br>\n",
    "3- We also need a vector for unknown characters.<br>\n",
    "4- Finally we need our dataset in character format.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by updating the embedding matrix\n",
    "\n",
    "# Add 'linebreak' vector to the embeddings_index\n",
    "linebreak_vector = 0\n",
    "for char_vector in embeddings_index.values():\n",
    "    linebreak_vector += char_vector\n",
    "linebreak_vector /= len(embeddings_index)\n",
    "\n",
    "# Add 'unknown' vector to the embeddings_index\n",
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "unk_vector = np.random.normal(mu, sigma, 300)\n",
    "\n",
    "# Add 'space' vector to the embeddings_index\n",
    "mu, sigma = 0, 0.2 # mean and standard deviation\n",
    "space_vector = np.random.normal(mu, sigma, 300)\n",
    "\n",
    "embeddings_index['\\n'] = linebreak_vector\n",
    "embeddings_index['UNK'] = unk_vector\n",
    "embeddings_index[' '] = space_vector\n",
    "\n",
    "# also update our ultimate character dictionary\n",
    "ultimate_alphabet.append(\"UNK\")\n",
    "ultimate_alphabet.append(\" \")\n",
    "ultimate_alphabet.append(\"\\n\")\n",
    "\n",
    "vocab_size = len(ultimate_alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, '/': 2, '8': 3, 'â': 4, 'ב': 5, 'ï': 6, '&': 7, 'ã': 8, 'ŋ': 9, 'w': 10, 'h': 11, '¿': 12, '|': 13, '1': 14, 'u': 15, 'm': 16, 'f': 17, ':': 18, '\"': 19, 'я': 20, 'θ': 21, 'ο': 22, 'ê': 23, 'σ': 24, 'д': 25, '}': 26, '`': 27, 'γ': 28, 'j': 29, 'с': 30, 'π': 31, 'ζ': 32, '_': 33, 'ν': 34, 'τ': 35, 'β': 36, 'ç': 37, 'ו': 38, 'æ': 39, '-': 40, '“': 41, 'ю': 42, 'м': 43, '(': 44, 'п': 45, 'k': 46, 'l': 47, '@': 48, 'p': 49, 'ч': 50, 'ü': 51, 'ð': 52, 'а': 53, 'г': 54, '2': 55, '\\\\': 56, 'd': 57, 'á': 58, '3': 59, 'ó': 60, 'e': 61, 'ā': 62, 'λ': 63, 'ω': 64, 'ὁ': 65, 'z': 66, 'þ': 67, 'g': 68, '0': 69, 'ס': 70, 'ь': 71, '*': 72, 'ă': 73, 'н': 74, 'л': 75, 'ξ': 76, '<': 77, 'ä': 78, '—': 79, 'ε': 80, 'κ': 81, 'э': 82, '¡': 83, '{': 84, 'é': 85, 'φ': 86, 'í': 87, 'ú': 88, 'à': 89, 'ש': 90, 'α': 91, 'ñ': 92, 'η': 93, '?': 94, '[': 95, 'o': 96, ')': 97, 'c': 98, 'ý': 99, 'ö': 100, 'ò': 101, 'v': 102, 'ד': 103, 'ι': 104, 'î': 105, 'q': 106, 'י': 107, '%': 108, 'μ': 109, 'x': 110, '+': 111, '!': 112, 'è': 113, 'и': 114, 'χ': 115, '”': 116, 'δ': 117, '~': 118, 'a': 119, 'i': 120, 't': 121, 'ш': 122, 'y': 123, 'n': 124, 'к': 125, ',': 126, 'å': 127, '#': 128, '5': 129, '9': 130, 'œ': 131, '…': 132, 'r': 133, '$': 134, '=': 135, 'א': 136, 'ר': 137, 'ה': 138, 'е': 139, 'т': 140, 'ρ': 141, '>': 142, ']': 143, '‘': 144, 's': 145, 'š': 146, 'υ': 147, 'b': 148, '’': 149, '4': 150, \"'\": 151, 'û': 152, '6': 153, '^': 154, '–': 155, '7': 156, ';': 157, 'UNK': 158, ' ': 159, '\\n': 160}\n"
     ]
    }
   ],
   "source": [
    "# convert our ultimate_alphabet to a dictionary\n",
    "ultimate_alphabet_dictionary = dict()\n",
    "for i, char in enumerate(ultimate_alphabet):\n",
    "    ultimate_alphabet_dictionary[char] = i + 1\n",
    "print(ultimate_alphabet_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding weights\n",
    "embedding_dim = 300 # since we have selected the 300-dimensional GloVe embedding\n",
    "embedding_matrix = np.zeros((vocab_size+1, embedding_dim)) # placeholder for out matrix, with padding included\n",
    "for char, i in ultimate_alphabet_dictionary.items(): \n",
    "    embedding_vector = embeddings_index.get(char) # if not find in the dict, return None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else: # For the unknown word in tk.word_index, assign UNK vector\n",
    "        embedding_vector = embeddings_index.get('UNK')\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.25589997e-01  1.36299999e-02  1.03060000e-01 -1.01230003e-01\n",
      "   9.81279984e-02  1.36270002e-01 -1.07210003e-01  2.36970007e-01\n",
      "   3.28700006e-01 -1.67850006e+00  2.23930001e-01  1.24090001e-01\n",
      "  -8.67080018e-02  3.30100000e-01  3.43750000e-01 -8.75819998e-04\n",
      "  -2.96579987e-01  2.44169995e-01 -1.15920000e-01 -3.57419997e-02\n",
      "  -1.08300000e-02  2.07760006e-01  2.92849988e-01 -7.34909996e-02\n",
      "  -1.85980007e-01 -2.00900003e-01 -9.53660011e-02  6.37320010e-03\n",
      "  -1.36199996e-01  9.20279995e-02 -3.99570018e-02  1.90270007e-01\n",
      "  -1.04560003e-01  2.76700011e-03 -7.17419982e-01 -1.29150003e-01\n",
      "  -1.34510000e-03  2.70020008e-01 -5.30229993e-02  2.21479997e-01\n",
      "   1.38809994e-01 -1.50509998e-01 -1.91499993e-01  1.64020002e-01\n",
      "   9.74840000e-02  5.68410009e-02  3.97890002e-01  4.07249987e-01\n",
      "   1.48019999e-01  2.15690002e-01 -1.06710002e-01 -1.02320001e-01\n",
      "   2.48099994e-02 -2.21000001e-01 -1.07199997e-02  1.42340004e-01\n",
      "  -2.82420009e-01  1.92540005e-01  8.67199972e-02 -3.89699996e-01\n",
      "   1.13210000e-01  1.37790001e-03  6.40089996e-03 -1.62059993e-01\n",
      "  -8.21529999e-02 -5.53969979e-01  3.67890000e-01 -4.01590019e-03\n",
      "   2.07100004e-01 -3.71569991e-01  2.51349986e-01 -1.95439994e-01\n",
      "  -4.70589995e-02  1.71550006e-01 -2.40360007e-01 -4.60859984e-02\n",
      "   1.94289997e-01 -1.89390004e-01 -7.19740009e-03  6.94810003e-02\n",
      "   5.91749996e-02 -1.75850004e-01  1.06530003e-01  1.69330001e-01\n",
      "  -3.61220017e-02  2.99110003e-02 -1.18299998e-01  1.39160007e-01\n",
      "  -3.79510000e-02  1.06899999e-01 -2.60690004e-01 -1.03069998e-01\n",
      "  -1.22720003e-01 -1.50319993e-01 -4.24089991e-02  1.33539997e-02\n",
      "  -2.85100013e-01  1.12480000e-02  1.60730004e-01 -1.63839996e-01\n",
      "   2.12329999e-01 -1.84760004e-01 -9.08739981e-04  6.66870028e-02\n",
      "   1.69180006e-01 -3.50039989e-01  9.90160033e-02  4.63930011e-01\n",
      "  -1.94619998e-01  1.03459999e-01 -2.56680012e-01 -3.65159988e-01\n",
      "  -1.89630002e-01 -2.19329998e-01  2.46339999e-02  6.56270012e-02\n",
      "  -1.11199997e-01 -1.64000005e-01  1.08740004e-02 -8.46880004e-02\n",
      "  -1.49230003e-01 -7.02230036e-02  2.88869999e-02  8.34970027e-02\n",
      "  -1.61930006e-02 -2.49259989e-03  1.71859995e-01  9.87489987e-03\n",
      "   8.02370012e-02  1.47740006e-01  4.32059988e-02  2.77159989e-01\n",
      "   5.76969981e-01 -4.12969999e-02  1.27649993e-01 -9.15170014e-02\n",
      "   1.41320005e-01  8.75789970e-02  9.32239965e-02  1.53460000e-02\n",
      "  -1.98559999e-01  1.72770005e-02 -1.07079998e-01 -1.30589996e-02\n",
      "  -3.72269988e-01  7.85679966e-02  1.66769996e-01 -1.53589994e-01\n",
      "  -3.32940012e-01  3.69860008e-02  1.16970003e-01  3.97810005e-02\n",
      "   3.84639986e-02 -1.62469998e-01  4.12800014e-01 -7.74910003e-02\n",
      "   4.54900004e-02  1.13300003e-01  8.21769983e-03 -2.50519991e-01\n",
      "   7.09659979e-02 -1.13880001e-01 -1.15029998e-01 -1.10140003e-01\n",
      "   1.04989998e-01  1.58779994e-01 -2.70229995e-01 -1.10060005e-02\n",
      "   7.60569994e-04  3.39020014e-01  2.55640000e-01  1.63420007e-01\n",
      "  -5.60190022e-01  1.30549997e-01  7.63109997e-02 -2.83339992e-02\n",
      "   2.87209988e-01 -2.78440006e-02 -1.15610003e-01  3.49249989e-01\n",
      "  -1.24200001e-01  2.14049995e-01  2.41160005e-01 -3.13429981e-02\n",
      "   1.09130003e-01 -2.47549996e-01 -4.54289988e-02 -8.21779966e-02\n",
      "  -1.88309997e-01  1.84459999e-01 -9.70740020e-02  3.23949993e-01\n",
      "   1.06579997e-01 -2.66759992e-01 -2.73110002e-01  1.71809997e-02\n",
      "   2.57959992e-01 -2.80479997e-01  3.07900012e-01 -2.17999995e-01\n",
      "   8.74149978e-01 -1.22970000e-01  1.09909996e-01 -2.97969997e-01\n",
      "   1.33939996e-01  1.06150001e-01 -1.07890002e-01 -3.59759986e-01\n",
      "  -1.83109999e-01 -4.51330006e-01  3.49670015e-02 -1.98469996e-01\n",
      "   2.19650000e-01  8.15199986e-02  2.58100003e-01  4.01730016e-02\n",
      "   3.13940011e-02  1.90689996e-01  7.58000016e-02 -6.06379993e-02\n",
      "   2.07389995e-01  9.83900018e-03 -2.69300014e-01  6.65149987e-02\n",
      "  -1.07110001e-01  5.99159999e-03  2.32840002e-01 -5.86629994e-02\n",
      "   9.89930034e-02 -8.14640000e-02  6.70040026e-02 -1.43050000e-01\n",
      "   2.55059987e-01 -3.19709986e-01 -3.10699996e-02 -9.24509987e-02\n",
      "   2.94400007e-01  2.89469987e-01 -5.98040000e-02  2.42860004e-01\n",
      "  -1.67549998e-01  4.20310013e-02  5.12610018e-01  2.45250002e-01\n",
      "  -6.59829974e-01  6.24560006e-02  5.22040017e-02 -2.57169995e-02\n",
      "  -8.06130022e-02  8.08689967e-02  2.28210002e-01 -1.02169998e-01\n",
      "  -2.07190007e-01 -1.21229999e-02  3.49159986e-01  8.65269974e-02\n",
      "   6.62880018e-02 -9.98279974e-02  2.58430004e-01  1.19429998e-01\n",
      "  -1.36669993e-01 -4.39619988e-01  2.37039998e-01  3.12959999e-02\n",
      "   7.47009963e-02 -2.23869994e-01  7.81620014e-03 -1.90160006e-01\n",
      "   4.44439985e-02  2.01910004e-01 -2.08140001e-01 -2.83820003e-01\n",
      "   1.04269996e-01 -2.10979998e-01  1.88649997e-01  3.16590011e-01\n",
      "  -2.07529998e+00 -7.10449964e-02  5.24190009e-01  5.60230017e-02\n",
      "  -2.52950013e-01 -6.21679984e-02 -1.09889999e-01 -3.57549995e-01\n",
      "  -7.92440027e-02  3.74720007e-01 -2.83529997e-01  1.63369998e-01\n",
      "   1.11649998e-01 -9.80020016e-02  6.01480007e-02 -1.56189993e-01\n",
      "  -1.19489998e-01  2.34449998e-01  8.13670009e-02  2.46179998e-01\n",
      "  -1.52419999e-01 -3.42240006e-01 -2.23939996e-02  1.36840001e-01]\n",
      " [ 1.78130001e-01 -4.11150008e-01 -5.10349989e-01  2.44560003e-01\n",
      "   9.31319967e-02  4.86790001e-01 -6.19009972e-01  5.97299993e-01\n",
      "  -9.88619998e-02 -8.62600029e-01  1.80710003e-01 -9.77620035e-02\n",
      "   1.01920001e-01  1.21990001e+00 -1.43240005e-01  5.43129981e-01\n",
      "   3.25580016e-02  3.15519989e-01 -1.77890003e-01  5.80860019e-01\n",
      "  -2.65760005e-01 -1.16360001e-01  2.78210014e-01 -5.18819988e-02\n",
      "   6.56920016e-01  2.66669989e-01  5.91310024e-01  1.86059996e-01\n",
      "   4.21929985e-01 -2.93689996e-01 -3.11839998e-01 -3.06529999e-02\n",
      "  -4.77730006e-01  7.88879991e-01 -1.02600002e+00 -2.47949995e-02\n",
      "  -4.68940008e-03 -4.40939993e-01 -5.75840008e-03  2.41679996e-01\n",
      "  -3.79009992e-01  7.06359968e-02 -1.65250003e-01  3.78859997e-01\n",
      "  -3.84779990e-01 -3.69269997e-01  5.32069981e-01 -3.62199992e-01\n",
      "   5.16149998e-01  3.20939988e-01  3.43829989e-01  4.08120006e-01\n",
      "  -9.39719975e-02 -7.31679976e-01 -9.71010029e-02  6.45290017e-01\n",
      "   4.35449988e-01 -1.55959994e-01  3.31200004e-01 -2.96500009e-02\n",
      "   2.08260000e-01  1.80790007e-01 -1.57729998e-01  5.92769980e-01\n",
      "  -9.81810018e-02  4.41579998e-01  2.71649987e-01 -1.11510001e-01\n",
      "   3.79079998e-01 -2.58010000e-01  1.43720001e-01  1.71010002e-01\n",
      "  -1.12640001e-01  2.19899993e-02 -8.64430010e-01  1.42230004e-01\n",
      "   1.01129997e+00 -2.80620009e-01  3.40530008e-01 -1.68129995e-01\n",
      "   1.63790002e-01  1.57609999e-01  5.26110008e-02  2.09669992e-02\n",
      "  -1.01039998e-01 -4.76330012e-01  8.35480019e-02  6.12640023e-01\n",
      "  -1.59529999e-01  1.25320002e-01 -3.17119986e-01 -4.29500014e-01\n",
      "  -5.27610004e-01 -3.02509993e-01  5.72130024e-01  1.76420003e-01\n",
      "  -6.20420016e-02 -1.39750004e-01  8.97180021e-01 -8.81449997e-01\n",
      "  -3.07720006e-01 -6.61989987e-01  5.87779999e-01 -2.93249995e-01\n",
      "   4.26760018e-02 -7.63429999e-01  4.68080014e-01  1.42330003e+00\n",
      "   2.52020001e-01 -4.34359998e-01  1.07830000e+00 -5.44089973e-01\n",
      "  -4.92040008e-01 -1.10129997e-01  3.05979997e-01 -8.06780010e-02\n",
      "  -1.28879994e-01  7.25470006e-01 -1.13070004e-01 -1.53999999e-01\n",
      "  -2.73079991e-01  4.40310001e-01  1.02180004e+00  1.53329998e-01\n",
      "   2.63679996e-02  2.72150010e-01 -7.74070024e-01  7.05540001e-01\n",
      "  -1.93379998e-01 -2.65450001e-01 -3.06890011e-01  3.60179991e-01\n",
      "   5.83040006e-02  2.10339993e-01 -6.86409995e-02 -7.89089978e-01\n",
      "  -6.68420017e-01 -3.73179987e-02  2.97170013e-01  4.03200001e-01\n",
      "  -5.79140007e-01 -2.78409988e-01  2.41180003e-01  2.09619999e-01\n",
      "  -6.82299972e-01  1.03509998e+00 -5.89730024e-01 -2.57719994e-01\n",
      "   6.31940007e-01  4.38180000e-01  1.31379998e+00  2.25720003e-01\n",
      "   5.45499980e-01  3.77700001e-01  7.38439977e-01  1.20940000e-01\n",
      "   4.93310004e-01 -1.27529994e-01 -1.99359998e-01  1.21880002e-01\n",
      "   3.92150015e-01 -2.32270002e-01 -1.09449998e-01 -3.59120011e-01\n",
      "   5.68370000e-02  5.64769983e-01  2.34390005e-01  1.67909995e-01\n",
      "  -3.26539993e-01  1.54649997e-02 -2.67740011e-01  5.07120013e-01\n",
      "   1.08949997e-01  6.11230016e-01  1.68329999e-01 -6.18369997e-01\n",
      "  -5.07019982e-02 -7.15380013e-01  2.54119992e-01  1.40090004e-01\n",
      "  -4.75129992e-01  2.03899994e-01 -1.10280000e-01  1.46640003e-01\n",
      "   3.10310006e-01 -7.28739977e-01 -8.81920010e-02 -2.84880012e-01\n",
      "   4.75039989e-01  7.15409994e-01 -5.54939985e-01  8.77709985e-02\n",
      "  -2.78869987e-01  2.35990003e-01 -5.77960014e-02 -2.94220001e-01\n",
      "   1.37390006e+00 -2.38670006e-01  1.02750003e+00  1.03589997e-01\n",
      "   6.80729985e-01 -6.07840002e-01 -3.58310014e-01  7.96760023e-02\n",
      "   3.39700013e-01  4.14600015e-01 -1.09399998e+00 -1.06490001e-01\n",
      "  -2.69080013e-01 -4.55150008e-01 -7.10269988e-01 -9.97839987e-01\n",
      "  -3.11010003e-01  5.17899990e-01  1.84589997e-01  4.67020005e-01\n",
      "  -6.57689989e-01  7.81660020e-01  5.41620016e-01 -6.10250011e-02\n",
      "   9.56699997e-02 -2.06660002e-01  4.28799987e-02  5.51119983e-01\n",
      "   1.69239998e-01 -3.37289989e-01 -3.82540002e-02 -2.16279998e-01\n",
      "  -2.77040005e-01  1.57539994e-01 -5.70410006e-02 -9.79629979e-02\n",
      "   4.46599990e-01 -5.64090014e-01  1.00759995e+00  1.11489996e-01\n",
      "  -2.03120001e-02  4.21370000e-01 -1.75830007e-01  2.95569986e-01\n",
      "  -5.45310020e-01 -1.11560002e-01  2.98750013e-01 -1.34440005e-01\n",
      "   1.48650005e-01  1.34599996e+00 -3.41670007e-01 -8.57700035e-02\n",
      "  -2.11270005e-02 -2.02270001e-01 -2.02450007e-02  1.43790007e-01\n",
      "   7.43160024e-02  2.83580005e-01 -3.33099991e-01  9.72430035e-02\n",
      "  -3.45789999e-01 -7.40140021e-01  6.89920008e-01  2.90060014e-01\n",
      "   1.55729994e-01 -5.34319997e-01 -7.75309978e-03 -2.56480008e-01\n",
      "  -7.94069991e-02  4.78179991e-01  5.17520010e-01  1.17180002e+00\n",
      "  -5.22979975e-01  6.97000027e-01  3.43609989e-01 -9.18079987e-02\n",
      "   9.12800014e-01 -1.14950001e+00 -2.27009997e-01  3.94210011e-01\n",
      "  -5.09090006e-01  2.50539988e-01  3.29829991e-01 -2.76520014e-01\n",
      "  -3.78630012e-01  1.17619999e-01 -5.83039999e-01 -4.20749992e-01\n",
      "   2.77680010e-01  7.47959971e-01  5.80949988e-03  4.10079986e-01\n",
      "  -2.54390001e-01 -5.88050008e-01 -1.17349997e-01  1.40009999e-01\n",
      "   6.11970015e-02  5.59910014e-02  4.50809985e-01 -4.91669998e-02\n",
      "   4.42229986e-01 -2.78369993e-01 -5.62510014e-01  9.44470018e-02]]\n"
     ]
    }
   ],
   "source": [
    "# see examples of the first three items, namely padding, '.' and '/'\n",
    "print(embedding_matrix[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then convert our dataset to character format with lowercase letters only\n",
    "\n",
    "# use a split function that takes a song lyrics, extracts all the individual characters and converts them into lowercase versions\n",
    "def split(word): \n",
    "    return [char.lower() for char in word]  \n",
    "\n",
    "x_splitted = list() # a new list for lyrics of splitted characters\n",
    "for lyric in x:\n",
    "    splitted_lyric = split(lyric)\n",
    "    splitted_lyric_indices = list()\n",
    "    for char in splitted_lyric:\n",
    "        try:\n",
    "            splitted_lyric_indices.append(ultimate_alphabet_dictionary[char])\n",
    "        except:\n",
    "            splitted_lyric_indices.append(ultimate_alphabet_dictionary['UNK'])\n",
    "    x_splitted.append(splitted_lyric_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the sub-word level data looks like in terms of example lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5]_True\n"
     ]
    }
   ],
   "source": [
    "print(str([3,4,5])+\"_\"+str(tunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  11111\n",
      "The min length is:  15\n",
      "The average length is:  1155.1754166666667\n"
     ]
    }
   ],
   "source": [
    "# See sub-word level length\n",
    "length = [len(lyric) for lyric in x_splitted]\n",
    "print('The max length is: ', max(length))\n",
    "print('The min length is: ', min(length))\n",
    "print('The average length is: ', sum(length)/len(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ready for the model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 11111)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded_sentences = pad_sequences(x_splitted, maxlen=max(length), padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "padded_sentences = np.array(padded_sentences)\n",
    "padded_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is time to split our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(23) # !!!!!! use the same seed value in all dataset split versions !!!!!!\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_artist))) \n",
    "# shuffle all inputs with the same indices\n",
    "x_shuffled = padded_sentences[shuffle_indices]\n",
    "y_artist_shuffled = y_artist[shuffle_indices]\n",
    "y_genre_shuffled = y_genre[shuffle_indices]\n",
    "\n",
    "# form count dictionaries for both artist and genre labels\n",
    "# first, artist labels\n",
    "artist_label_count_dict = dict()\n",
    "for i in range(120):\n",
    "    artist_label_count_dict[i] = 0\n",
    "# then also for genre labels\n",
    "genre_label_count_dict = dict()\n",
    "for i in range(12):\n",
    "    genre_label_count_dict[i] = 0\n",
    "    \n",
    "'''Now, we'll go through the data examples one by one. If each artist label occurs less than 80 times, the sample 
will belong to the training set; occurances between 81th and 90th times will go to the validation set and the 
occurances between 91th and 100th times (last 10 occurances) will go to the test set.
For genre labels, any genre label occuring less than 800 times will go to the training set; occurances between 801th and 
900th times will go to the validation set and the occurances between 901th and 1000th times (last 100 occurances) will 
go to the test set.'''\n",
    "\n",
    "# create training, validation and test sets with equal distributions of artists and genres\n",
    "x_tr_artist_equal, x_val_artist_equal, x_te_artist_equal = list(), list(), list()\n",
    "x_tr_genre_equal, x_val_genre_equal, x_te_genre_equal = list(), list(), list()\n",
    "y_tr_artist_equal, y_val_artist_equal, y_te_artist_equal = list(), list(), list()\n",
    "y_tr_genre_equal, y_val_genre_equal, y_te_genre_equal = list(), list(), list()\n",
    "\n",
    "for sample, art_label, gen_label in zip(x_shuffled, y_artist_shuffled, y_genre_shuffled):\n",
    "    artist_label_index = np.argmax(art_label, axis=-1)\n",
    "    genre_label_index = np.argmax(gen_label, axis=-1)\n",
    "    # for artist labels\n",
    "    if artist_label_count_dict[artist_label_index] < 80:\n",
    "        x_tr_artist_equal.append(sample)\n",
    "        y_tr_artist_equal.append(art_label)\n",
    "    elif 80 <= artist_label_count_dict[artist_label_index] < 90:\n",
    "        x_val_artist_equal.append(sample)\n",
    "        y_val_artist_equal.append(art_label)\n",
    "    elif 90 <= artist_label_count_dict[artist_label_index] < 100:\n",
    "        x_te_artist_equal.append(sample)\n",
    "        y_te_artist_equal.append(art_label)\n",
    "    else:\n",
    "        print(\"There is an error with artist counts!\")\n",
    "    artist_label_count_dict[artist_label_index] += 1\n",
    "        \n",
    "    # for genre labels\n",
    "    if genre_label_count_dict[genre_label_index] < 800:\n",
    "        x_tr_genre_equal.append(sample)\n",
    "        y_tr_genre_equal.append(gen_label)\n",
    "    elif 800 <= genre_label_count_dict[genre_label_index] < 900:\n",
    "        x_val_genre_equal.append(sample)\n",
    "        y_val_genre_equal.append(gen_label)\n",
    "    elif 900 <= genre_label_count_dict[genre_label_index] < 1000:\n",
    "        x_te_genre_equal.append(sample)\n",
    "        y_te_genre_equal.append(gen_label)\n",
    "    else:\n",
    "        print(\"There is an error with genre counts!\")\n",
    "    genre_label_count_dict[genre_label_index] += 1\n",
    "\n",
    "        \n",
    "        \n",
    "# turn the output datasets in np arrays\n",
    "x_tr_artist_equal = np.array(x_tr_artist_equal)\n",
    "x_val_artist_equal = np.array(x_val_artist_equal)\n",
    "x_te_artist_equal = np.array(x_te_artist_equal)\n",
    "x_tr_genre_equal = np.array(x_tr_genre_equal)\n",
    "x_val_genre_equal = np.array(x_val_genre_equal)\n",
    "x_te_genre_equal = np.array(x_te_genre_equal)\n",
    "y_tr_artist_equal = np.array(y_tr_artist_equal)\n",
    "y_val_artist_equal = np.array(y_val_artist_equal)\n",
    "y_te_artist_equal = np.array(y_te_artist_equal)\n",
    "y_tr_genre_equal = np.array(y_tr_genre_equal)\n",
    "y_val_genre_equal = np.array(y_val_genre_equal)\n",
    "y_te_genre_equal  = np.array(y_te_genre_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all important variables in pickle files to a folder specific to the character model\n",
    "def writePickleChar(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/character/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "\n",
    "#writePickleChar(embedding_matrix, \"embedding_weights\")\n",
    "writePickleChar(x_tr_artist_equal, \"x_tr_artist_equal\")\n",
    "writePickleChar(x_val_artist_equal, \"x_val_artist_equal\")\n",
    "writePickleChar(x_te_artist_equal, \"x_te_artist_equal\")\n",
    "writePickleChar(x_tr_genre_equal, \"x_tr_genre_equal\")\n",
    "writePickleChar(x_val_genre_equal, \"x_val_genre_equal\")\n",
    "writePickleChar(x_te_genre_equal, \"x_te_genre_equal\")\n",
    "writePickleChar(y_tr_artist_equal, \"y_tr_artist_equal\")\n",
    "writePickleChar(y_val_artist_equal, \"y_val_artist_equal\")\n",
    "writePickleChar(y_te_artist_equal, \"y_te_artist_equal\")\n",
    "writePickleChar(y_tr_genre_equal, \"y_tr_genre_equal\")\n",
    "writePickleChar(y_val_genre_equal, \"y_val_genre_equal\")\n",
    "writePickleChar(y_te_genre_equal, \"y_te_genre_equal\")\n",
    "#writePickleChar(vocab_size, \"vocab_size\")\n",
    "#writePickleChar(length, \"sequence_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Initial Incorrect Split Code\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Shuffle data\n",
    "np.random.seed(23) # !!!!!! use the same seed value in all dataset split versions !!!!!!\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_artist))) \n",
    "# shuffle all inputs with the same indices\n",
    "x_shuffled = padded_sentences[shuffle_indices]\n",
    "y_artist_shuffled = y_artist[shuffle_indices]\n",
    "y_genre_shuffled = y_genre[shuffle_indices]\n",
    "\n",
    "# Split training, validation and test sets with a ratio of 80-10-10\n",
    "training_slice = int(len(y_artist) * 0.8)\n",
    "validation_slice = int(len(y_artist) * 0.9)\n",
    "\n",
    "# training\n",
    "x_tr = x_shuffled[:training_slice]\n",
    "y_tr_artist = y_artist_shuffled[:training_slice]\n",
    "y_tr_genre = y_genre_shuffled[:training_slice]\n",
    "\n",
    "# validation\n",
    "x_val = x_shuffled[training_slice:validation_slice]\n",
    "y_val_artist = y_artist_shuffled[training_slice:validation_slice]\n",
    "y_val_genre = y_genre_shuffled[training_slice:validation_slice]\n",
    "\n",
    "# test\n",
    "x_te = x_shuffled[validation_slice:]\n",
    "y_te_artist = y_artist_shuffled[validation_slice:]\n",
    "y_te_genre = y_genre_shuffled[validation_slice:]\n",
    "\n",
    "print('Training data size is: ', x_tr.shape)\n",
    "print('Validation data size is: ', x_val.shape)\n",
    "print('Test data size is: ', x_te.shape)\n",
    "\n",
    "# store all important variables in pickle files to a folder specific to the character model\n",
    "def writePickleChar(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/character/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "\n",
    "writePickleChar(embedding_matrix, \"embedding_weights\")\n",
    "writePickleChar(x_tr, \"x_tr\")\n",
    "writePickleChar(x_te, \"x_te\")\n",
    "writePickleChar(x_val, \"x_val\")\n",
    "writePickleChar(y_tr_artist, \"y_tr_artist\")\n",
    "writePickleChar(y_tr_genre, \"y_tr_genre\")\n",
    "writePickleChar(y_te_artist, \"y_te_artist\")\n",
    "writePickleChar(y_te_genre, \"y_te_genre\")\n",
    "writePickleChar(y_val_artist, \"y_val_artist\")\n",
    "writePickleChar(y_val_genre, \"y_val_genre\")\n",
    "writePickleChar(vocab_size, \"vocab_size\")\n",
    "writePickleChar(length, \"sequence_length\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
