{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we'll prepare our dataset for the sub-word model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by using some pickle helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def writePickle(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "def readPickle(fname):\n",
    "    filename = \"pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "def readPicklePast(fname):\n",
    "    filename = \"../pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also load the complete sub_dataset from the csv file, and get all examples with their corresponding artist labels (categorical) and genre labels (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    data = pd.read_csv('sub_dataset.csv', header=None)\n",
    "    data = data.dropna()\n",
    "\n",
    "    x = data[2]\n",
    "    x = np.array(x)\n",
    "\n",
    "    y_artist = data[0] - 1\n",
    "    y_artist = to_categorical(y_artist)\n",
    "    \n",
    "    y_genre = data[1] - 1\n",
    "    y_genre = to_categorical(y_genre)\n",
    "    \n",
    "    return (x, y_artist, y_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_artist, y_genre = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also load the genre and artist id mapping dictionaries\n",
    "artist2id = readPickle(\"artist2id\")\n",
    "id2artist = readPickle(\"id2artist\")\n",
    "genre2id = readPickle(\"genre2id\")\n",
    "id2genre = readPickle(\"id2genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Someone wrote just yesterday\n",
      "\"Love's in need of love today\"\n",
      "Will there be none tomorrow\n",
      "\n",
      "Faded feelings, jaded news\n",
      "Where's the tender hearts\n",
      "That love romance, they\n",
      "Seem so few\n",
      "\n",
      "So, I write these words to say\n",
      "Take heart have faith\n",
      "In love's tomorrow\n",
      "Where our precious dreams\n",
      "Will all come true\n",
      "\n",
      "Heaven knows what I've\n",
      "Been through\n",
      "Searching for a heart that's true\n",
      "Is there one in a million?\n",
      "\n",
      "Though I've often failed the test\n",
      "Everyone must journey till\n",
      "That questioning is through\n",
      "\n",
      "Shine a star for me to see what\n",
      "Children see on Christmas morning\n",
      "Every hope and dream\n",
      "To be renewed\n",
      "\n",
      "Heaven only knows how I hope\n",
      "That chance of love is higher\n",
      "Than one in a million\n",
      "\n",
      "I'm a dreamer I confess\n",
      "But my dreams are only of\n",
      "The best that we can do\n",
      "\n",
      "So, I'll keep on counting sheep\n",
      "And will not sleep until the dawning\n",
      "\n",
      "Help me break the news, eternal\n",
      "Love just beat the blues\n",
      "\n",
      "Save me from no love\n",
      "(You oughta save me)\n",
      "Save me from no love\n",
      "Say sweet love has come\n",
      "And is not gone,\n",
      "There is all, that I want,\n",
      "This morning\n",
      "\n",
      "Save me from no love\n",
      "(You oughta save me)\n",
      "Save me from no love\n",
      "Say sweet love has come\n",
      "And will live on,\n",
      "Never gone, ever on,\n",
      "(Strong), eternally (forever)\n",
      "\n",
      "Now I've done the best I can, girl\n",
      "I need a helping hand\n",
      "Are you one in a million?\n",
      "\n",
      "People leave without good-byes\n",
      "Stay with me, for through your\n",
      "Eyes I see another truth\n",
      "\n",
      "You're the star and now I see what\n",
      "Children see on Christmas morning\n",
      "\n",
      "Stay and see it through the gift\n",
      "Of love returned to you\n",
      "\n",
      "\n",
      "I'm searchin' for love, baby,\n",
      "Been lookin' today\n",
      "Somewhere there's a love, baby,\n",
      "Do you know the way?\n",
      "\n",
      "\n",
      "Save me, save me\n",
      "You've got to save me\n",
      "Save me, save me\n",
      "Baby, you can save me\n",
      "Save me, save me\n",
      "You've got to save me\n",
      "From no love \n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Al Jarreau \n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] Jazz\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print(x[100],'\\n', y_artist[100], id2artist[1 + np.argmax(y_artist[100], axis=-1)],'\\n', \\\n",
    "      y_genre[100], id2genre[1 + np.argmax(y_genre[100], axis=-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll try to follow the *Kim (2014)* **Convolutional Neural Networks for Sentence Classification** paper for our model. Therefore below we clean our string from a function taken from the github page of the model implementation <br> <br>\n",
    "\n",
    "For song lyrics, tabs, line breaks, punctuation like \"...\" might be very important. Therefore later we might remove/modify this preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [clean_str(sen) for sen in x]\n",
    "x = [sen.split(\" \") for sen in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['someone', 'wrote', 'just', 'yesterday', 'love', \"'s\", 'in', 'need', 'of', 'love', 'today', 'will', 'there', 'be', 'none', 'tomorrow', 'faded', 'feelings', ',', 'jaded', 'news', 'where', \"'s\", 'the', 'tender', 'hearts', 'that', 'love', 'romance', ',', 'they', 'seem', 'so', 'few', 'so', ',', 'i', 'write', 'these', 'words', 'to', 'say', 'take', 'heart', 'have', 'faith', 'in', 'love', \"'s\", 'tomorrow', 'where', 'our', 'precious', 'dreams', 'will', 'all', 'come', 'true', 'heaven', 'knows', 'what', 'i', \"'ve\", 'been', 'through', 'searching', 'for', 'a', 'heart', 'that', \"'s\", 'true', 'is', 'there', 'one', 'in', 'a', 'million', '\\\\?', 'though', 'i', \"'ve\", 'often', 'failed', 'the', 'test', 'everyone', 'must', 'journey', 'till', 'that', 'questioning', 'is', 'through', 'shine', 'a', 'star', 'for', 'me', 'to', 'see', 'what', 'children', 'see', 'on', 'christmas', 'morning', 'every', 'hope', 'and', 'dream', 'to', 'be', 'renewed', 'heaven', 'only', 'knows', 'how', 'i', 'hope', 'that', 'chance', 'of', 'love', 'is', 'higher', 'than', 'one', 'in', 'a', 'million', \"i'm\", 'a', 'dreamer', 'i', 'confess', 'but', 'my', 'dreams', 'are', 'only', 'of', 'the', 'best', 'that', 'we', 'can', 'do', 'so', ',', 'i', \"'ll\", 'keep', 'on', 'counting', 'sheep', 'and', 'will', 'not', 'sleep', 'until', 'the', 'dawning', 'help', 'me', 'break', 'the', 'news', ',', 'eternal', 'love', 'just', 'beat', 'the', 'blues', 'save', 'me', 'from', 'no', 'love', '\\\\(', 'you', 'oughta', 'save', 'me', '\\\\)', 'save', 'me', 'from', 'no', 'love', 'say', 'sweet', 'love', 'has', 'come', 'and', 'is', 'not', 'gone', ',', 'there', 'is', 'all', ',', 'that', 'i', 'want', ',', 'this', 'morning', 'save', 'me', 'from', 'no', 'love', '\\\\(', 'you', 'oughta', 'save', 'me', '\\\\)', 'save', 'me', 'from', 'no', 'love', 'say', 'sweet', 'love', 'has', 'come', 'and', 'will', 'live', 'on', ',', 'never', 'gone', ',', 'ever', 'on', ',', '\\\\(', 'strong', '\\\\)', ',', 'eternally', '\\\\(', 'forever', '\\\\)', 'now', 'i', \"'ve\", 'done', 'the', 'best', 'i', 'can', ',', 'girl', 'i', 'need', 'a', 'helping', 'hand', 'are', 'you', 'one', 'in', 'a', 'million', '\\\\?', 'people', 'leave', 'without', 'good', 'byes', 'stay', 'with', 'me', ',', 'for', 'through', 'your', 'eyes', 'i', 'see', 'another', 'truth', 'you', \"'re\", 'the', 'star', 'and', 'now', 'i', 'see', 'what', 'children', 'see', 'on', 'christmas', 'morning', 'stay', 'and', 'see', 'it', 'through', 'the', 'gift', 'of', 'love', 'returned', 'to', 'you', \"i'm\", \"searchin'\", 'for', 'love', ',', 'baby', ',', 'been', \"lookin'\", 'today', 'somewhere', 'there', \"'s\", 'a', 'love', ',', 'baby', ',', 'do', 'you', 'know', 'the', 'way', '\\\\?', 'save', 'me', ',', 'save', 'me', 'you', \"'ve\", 'got', 'to', 'save', 'me', 'save', 'me', ',', 'save', 'me', 'baby', ',', 'you', 'can', 'save', 'me', 'save', 'me', ',', 'save', 'me', 'you', \"'ve\", 'got', 'to', 'save', 'me', 'from', 'no', 'love']\n"
     ]
    }
   ],
   "source": [
    "# compare the new example format with the raw version above\n",
    "print(x[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the data length properties look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2184\n",
      "2\n",
      "254.337\n"
     ]
    }
   ],
   "source": [
    "print(max([len(sentence) for sentence in x])) # max length\n",
    "print(min([len(sentence) for sentence in x])) # min length\n",
    "print(sum([len(sentence) for sentence in x])/len([len(sentence) for sentence in x])) # avg length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the maximum and the minimum length data examples are quite divergent from the average. This might result in a lot of padding for many examples. **We'll ignore this issue for now, but alternative models may reconsider cropping some data examples later on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From here on, we'll use a sub-word level embedding package retrieved from: https://github.com/bheinzerling/bpemb <br>\n",
    "#### We'll also prepare our data for keras implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sub-word level embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "# load English BPEmb model with default vocabulary size (10k) and 50-dimensional embeddings\n",
    "bpemb_en = BPEmb(lang=\"en\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a built-in function in the BPEMB library\n",
    "texts = [bpemb_en.encode(s) for s in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁carry'], ['▁each'], ['▁other'], [\"▁'\", 's'], ['▁bur', 'd', 'ens'], ['▁carry'], ['▁each'], ['▁other'], [\"▁'\", 's'], ['▁bur', 'd', 'ens'], ['▁and'], ['▁in'], ['▁this'], ['▁way'], ['▁you'], ['▁will'], ['▁fulf', 'ill'], ['▁the'], ['▁law'], ['▁of'], ['▁christ'], ['▁be'], ['▁devoted'], ['▁', '\\\\', '('], ['▁be'], ['▁devoted'], ['▁', '\\\\', ')'], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', '('], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', ')'], ['▁be'], ['▁devoted'], ['▁', '\\\\', '('], ['▁be'], ['▁devoted'], ['▁', '\\\\', ')'], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', '('], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', ')'], ['▁honor'], ['▁one'], ['▁another'], ['▁above'], ['▁y', 'ours', 'elves'], ['▁', '\\\\', '('], ['▁above'], ['▁y', 'ours', 'elves'], ['▁', '\\\\', ')'], ['▁never'], ['▁be'], ['▁lack', 'ing'], ['▁in'], ['▁ze', 'al'], ['▁', '\\\\', '('], ['▁never'], ['▁never'], ['▁,'], ['▁never'], ['▁,'], ['▁never'], ['▁be'], ['▁lack', 'ing'], ['▁in'], ['▁ze', 'al'], ['▁', '\\\\', ')'], ['▁keep'], ['▁', '\\\\', '('], ['▁keep'], ['▁', '\\\\', ')'], ['▁your'], ['▁spiritual'], ['▁', '\\\\', '('], ['▁your'], ['▁spiritual'], ['▁', '\\\\', ')'], ['▁f', 'erv', 'or'], ['▁keep'], ['▁', '\\\\', '('], ['▁keep'], ['▁', '\\\\', ')'], ['▁your'], ['▁', '\\\\', '('], ['▁your'], ['▁', '\\\\', ')'], ['▁spiritual'], ['▁', '\\\\', '('], ['▁spiritual'], ['▁', '\\\\', ')'], ['▁', '\\\\', '('], ['▁your'], ['▁', '\\\\', ')'], ['▁f', 'erv', 'or'], ['▁serving'], ['▁the'], ['▁lord'], ['▁love'], ['▁', '\\\\', '('], ['▁love'], ['▁', '\\\\', ')'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁serve'], ['▁', '\\\\', '('], ['▁serve'], ['▁', '\\\\', ')'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁pray'], ['▁for'], ['▁each'], ['▁other'], ['▁', '\\\\', '('], ['▁each'], ['▁other'], ['▁', '\\\\', ')'], ['▁encoura', 'ge'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁and'], ['▁build'], ['▁each'], ['▁other'], ['▁up'], ['▁be'], ['▁devoted'], ['▁', '\\\\', '('], ['▁be'], ['▁devoted'], ['▁', '\\\\', ')'], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', '('], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', ')'], ['▁be'], ['▁devoted'], ['▁', '\\\\', '('], ['▁be'], ['▁devoted'], ['▁', '\\\\', ')'], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', '('], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', ')'], ['▁be'], ['▁devoted'], ['▁', '\\\\', '('], ['▁be'], ['▁devoted'], ['▁', '\\\\', ')'], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', '('], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', ')'], ['▁be'], ['▁devoted'], ['▁', '\\\\', '('], ['▁be'], ['▁devoted'], ['▁', '\\\\', ')'], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', '('], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁', '\\\\', ')'], ['▁be'], ['▁devoted'], ['▁', '\\\\', '('], ['▁be'], ['▁devoted'], ['▁', '\\\\', ')'], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', '('], ['▁to'], ['▁one'], ['▁another'], ['▁', '\\\\', ')'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁script', 'ural'], ['▁reference'], ['▁be'], ['▁devoted'], ['▁to'], ['▁one'], ['▁another'], ['▁in'], ['▁brother', 'ly'], ['▁love'], ['▁honor'], ['▁one'], ['▁another'], ['▁above'], ['▁y', 'ours', 'elves'], ['▁never'], ['▁be'], ['▁lack', 'ing'], ['▁in'], ['▁ze', 'al'], ['▁,'], ['▁but'], ['▁keep'], ['▁your'], ['▁spiritual'], ['▁f', 'erv', 'or'], ['▁,'], ['▁serving'], ['▁the'], ['▁lord'], ['▁romans'], ['▁00'], ['▁00'], ['▁00']]\n"
     ]
    }
   ],
   "source": [
    "# show an example\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'the' is: 8 \n",
      "Index of unknown is: 4993\n",
      "The vocabulary length is: 10000\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary dictionary where each token is mapped to an index value\n",
    "vocabulary = {}\n",
    "for index, token in enumerate(bpemb_en.words):\n",
    "    vocabulary[token] = index + 1\n",
    "\n",
    "print(\"Index of 'the' is:\", vocabulary['▁the'], \"\\nIndex of unknown is:\", vocabulary[\"unk\"])\n",
    "print(\"The vocabulary length is:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each subword to its index value with the following helper function\n",
    "def subword2index(texts, vocabulary):\n",
    "    sentences = []\n",
    "    for text in texts:\n",
    "        one_line = []\n",
    "        for sub_word in text:\n",
    "            for word in sub_word:\n",
    "                if word not in vocabulary.keys():\n",
    "                    one_line.append(vocabulary['unk'])\n",
    "                else:\n",
    "                    one_line.append(vocabulary[word])\n",
    "        sentences.append(one_line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = subword2index(texts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5025, 883, 303, 882, 9921, 1492, 9924, 305, 5025, 883, 303, 882, 9921, 1492, 9924, 305, 35, 27, 216, 1105, 1221, 506, 9265, 142, 8, 873, 28, 1060, 87, 8674, 9913, 4993, 9943, 87, 8674, 9913, 4993, 9942, 43, 273, 1075, 9913, 4993, 9943, 43, 273, 1075, 9913, 4993, 9942, 27, 1724, 75, 2154, 9913, 4993, 9943, 27, 1724, 75, 2154, 9913, 4993, 9942, 87, 8674, 9913, 4993, 9943, 87, 8674, 9913, 4993, 9942, 43, 273, 1075, 9913, 4993, 9943, 43, 273, 1075, 9913, 4993, 9942, 27, 1724, 75, 2154, 9913, 4993, 9943, 27, 1724, 75, 2154, 9913, 4993, 9942, 2853, 273, 1075, 2208, 156, 2006, 2898, 9913, 4993, 9943, 2208, 156, 2006, 2898, 9913, 4993, 9942, 1657, 87, 2856, 38, 27, 2449, 31, 9913, 4993, 9943, 1657, 1657, 1750, 1657, 1750, 1657, 87, 2856, 38, 27, 2449, 31, 9913, 4993, 9942, 2902, 9913, 4993, 9943, 2902, 9913, 4993, 9942, 3286, 6071, 9913, 4993, 9943, 3286, 6071, 9913, 4993, 9942, 22, 914, 23, 2902, 9913, 4993, 9943, 2902, 9913, 4993, 9942, 3286, 9913, 4993, 9943, 3286, 9913, 4993, 9942, 6071, 9913, 4993, 9943, 6071, 9913, 4993, 9942, 9913, 4993, 9943, 3286, 9913, 4993, 9942, 22, 914, 23, 3918, 8, 2243, 2154, 9913, 4993, 9943, 2154, 9913, 4993, 9942, 273, 1075, 9913, 4993, 9943, 273, 1075, 9913, 4993, 9942, 3735, 9913, 4993, 9943, 3735, 9913, 4993, 9942, 273, 1075, 9913, 4993, 9943, 273, 1075, 9913, 4993, 9942, 5780, 73, 883, 303, 9913, 4993, 9943, 883, 303, 9913, 4993, 9942, 4286, 112, 273, 1075, 9913, 4993, 9943, 273, 1075, 9913, 4993, 9942, 35, 915, 883, 303, 366, 87, 8674, 9913, 4993, 9943, 87, 8674, 9913, 4993, 9942, 43, 273, 1075, 9913, 4993, 9943, 43, 273, 1075, 9913, 4993, 9942, 27, 1724, 75, 2154, 9913, 4993, 9943, 27, 1724, 75, 2154, 9913, 4993, 9942, 87, 8674, 9913, 4993, 9943, 87, 8674, 9913, 4993, 9942, 43, 273, 1075, 9913, 4993, 9943, 43, 273, 1075, 9913, 4993, 9942, 27, 1724, 75, 2154, 9913, 4993, 9943, 27, 1724, 75, 2154, 9913, 4993, 9942, 87, 8674, 9913, 4993, 9943, 87, 8674, 9913, 4993, 9942, 43, 273, 1075, 9913, 4993, 9943, 43, 273, 1075, 9913, 4993, 9942, 27, 1724, 75, 2154, 9913, 4993, 9943, 27, 1724, 75, 2154, 9913, 4993, 9942, 87, 8674, 9913, 4993, 9943, 87, 8674, 9913, 4993, 9942, 43, 273, 1075, 9913, 4993, 9943, 43, 273, 1075, 9913, 4993, 9942, 27, 1724, 75, 2154, 9913, 4993, 9943, 27, 1724, 75, 2154, 9913, 4993, 9942, 87, 8674, 9913, 4993, 9943, 87, 8674, 9913, 4993, 9942, 43, 273, 1075, 9913, 4993, 9943, 43, 273, 1075, 9913, 4993, 9942, 27, 1724, 75, 2154, 4051, 842, 4048, 87, 8674, 43, 273, 1075, 27, 1724, 75, 2154, 2853, 273, 1075, 2208, 156, 2006, 2898, 1657, 87, 2856, 38, 27, 2449, 31, 1750, 277, 2902, 3286, 6071, 22, 914, 23, 1750, 3918, 8, 2243, 9671, 40, 40, 40], [387, 426, 8593, 49, 38, 9913, 4993, 9943, 387, 426, 8593, 49, 38, 9913, 4993, 9942, 746, 8, 1512, 9913, 4993, 9943, 746, 8, 1512, 9913, 4993, 9942, 131, 2861, 8, 2086, 75, 867, 9913, 4993, 9943, 8, 2086, 75, 867, 9913, 4993, 9942, 1711, 852, 269, 9938, 775, 387, 426, 5780, 38, 9913, 4993, 9943, 5780, 38, 9913, 4993, 9942, 73, 147, 976, 545, 147, 3363, 27, 2951, 5874, 683, 416, 1756, 102, 4459, 9913, 4993, 9943, 416, 1756, 102, 4459, 9913, 4993, 9942, 147, 2154, 9913, 4993, 9943, 510, 3265, 1750, 510, 770, 9913, 4993, 9942, 651, 3814, 30, 3857, 9913, 4993, 9943, 510, 526, 1750, 510, 3333, 9913, 4993, 9942, 147, 2154, 9913, 4993, 9943, 510, 3265, 1750, 510, 770, 9913, 4993, 9942, 651, 3814, 30, 3857, 9913, 4993, 9943, 510, 526, 1750, 510, 3333, 9913, 4993, 9942, 387, 426, 8593, 49, 38, 9913, 4993, 9943, 387, 426, 8593, 49, 38, 9913, 4993, 9942, 746, 8, 1512, 9913, 4993, 9943, 746, 8, 1512, 9913, 4993, 9942, 131, 2861, 8, 7824, 75, 867, 1711, 852, 269, 9938, 775, 387, 426, 5780, 38, 9913, 4993, 9943, 5780, 38, 9913, 4993, 9942, 372, 28, 147, 6466, 1162, 1221, 882, 1189, 87, 172, 4259, 35, 1613, 27, 147, 2154, 35, 387, 5780, 73, 1221, 98, 261, 8, 7784, 121, 1060, 1756, 27, 3286, 2728, 9921, 545, 4459, 35, 87, 6770, 43, 8, 3949, 28, 261, 1675, 1275, 28, 1741, 35, 387, 5780, 73, 1221, 98, 261, 8, 7784, 121, 1060, 1756, 27, 3286, 2728, 9921, 545, 4459, 35, 87, 6770, 43, 8, 3949, 28, 261, 1675, 1275, 28, 1741, 510, 3265, 1750, 510, 770, 1750, 510, 526, 1750, 510, 3333, 4051, 842, 4048, 73, 216, 3298, 387, 2538, 1242, 8593, 30, 43, 8, 1512, 28, 2951, 2243, 5193, 1060, 1750, 131, 2861, 8, 3395, 867, 27, 7824, 35, 2086, 81, 1145, 1750, 121, 84, 498, 3995, 1221, 1750, 1272, 43, 8, 6466, 1162, 28, 147, 880, 326, 1750, 43, 87, 3810, 229, 2657, 98, 2920, 545, 147, 3363, 27, 8, 5874, 237, 1750, 121, 1060, 438, 34, 2504, 27, 3286, 2728, 9921, 545, 4459, 121, 1221, 1750, 683, 172, 4259, 35, 1895, 19, 27, 2154, 1750, 438, 87, 2384, 43, 210, 13, 991, 9924, 98, 261, 8, 7784, 1295, 81, 8, 2099, 107, 35, 3078, 35, 6847, 35, 5309, 43, 2171, 8, 2154, 28, 1060, 193, 5898, 3857, 121, 1221, 438, 87, 6770, 98, 261, 8, 1675, 1275, 28, 1741, 999, 1162, 1019, 122, 40, 40]]\n"
     ]
    }
   ],
   "source": [
    "# show some examples\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the sub-word level data looks like in terms of example lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  3674\n",
      "The min length is:  3\n",
      "The average length is:  326.987\n"
     ]
    }
   ],
   "source": [
    "# See sub-word level length\n",
    "length = [len(sentence) for sentence in sentences]\n",
    "print('The max length is: ', max(length))\n",
    "print('The min length is: ', min(length))\n",
    "print('The average length is: ', sum(length)/len(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap is even bigger now!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ready for the model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 3674)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded_sentences = pad_sequences(sentences, maxlen=max(length), padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "padded_sentences = np.array(padded_sentences)\n",
    "padded_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is time to split our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(23) # !!!!!! use the same seed value in all dataset split versions !!!!!!\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_artist))) \n",
    "# shuffle all inputs with the same indices\n",
    "x_shuffled = padded_sentences[shuffle_indices]\n",
    "y_artist_shuffled = y_artist[shuffle_indices]\n",
    "y_genre_shuffled = y_genre[shuffle_indices]\n",
    "\n",
    "# form count dictionaries for both artist and genre labels\n",
    "# first, artist labels\n",
    "artist_label_count_dict = dict()\n",
    "for i in range(120):\n",
    "    artist_label_count_dict[i] = 0\n",
    "# then also for genre labels\n",
    "genre_label_count_dict = dict()\n",
    "for i in range(12):\n",
    "    genre_label_count_dict[i] = 0\n",
    "    \n",
    "'''Now, we'll go through the data examples one by one. If each artist label occurs less than 80 times, the sample \n",
    "will belong to the training set; occurances between 81th and 90th times will go to the validation set and the \n",
    "occurances between 91th and 100th times (last 10 occurances) will go to the test set.\n",
    "For genre labels, any genre label occuring less than 800 times will go to the training set; occurances between 801th and \n",
    "900th times will go to the validation set and the occurances between 901th and 1000th times (last 100 occurances) will \n",
    "go to the test set.'''\n",
    "\n",
    "# create training, validation and test sets with equal distributions of artists and genres\n",
    "x_tr_artist_equal, x_val_artist_equal, x_te_artist_equal = list(), list(), list()\n",
    "x_tr_genre_equal, x_val_genre_equal, x_te_genre_equal = list(), list(), list()\n",
    "y_tr_artist_equal, y_val_artist_equal, y_te_artist_equal = list(), list(), list()\n",
    "y_tr_genre_equal, y_val_genre_equal, y_te_genre_equal = list(), list(), list()\n",
    "\n",
    "for sample, art_label, gen_label in zip(x_shuffled, y_artist_shuffled, y_genre_shuffled):\n",
    "    artist_label_index = np.argmax(art_label, axis=-1)\n",
    "    genre_label_index = np.argmax(gen_label, axis=-1)\n",
    "    # for artist labels\n",
    "    if artist_label_count_dict[artist_label_index] < 80:\n",
    "        x_tr_artist_equal.append(sample)\n",
    "        y_tr_artist_equal.append(art_label)\n",
    "    elif 80 <= artist_label_count_dict[artist_label_index] < 90:\n",
    "        x_val_artist_equal.append(sample)\n",
    "        y_val_artist_equal.append(art_label)\n",
    "    elif 90 <= artist_label_count_dict[artist_label_index] < 100:\n",
    "        x_te_artist_equal.append(sample)\n",
    "        y_te_artist_equal.append(art_label)\n",
    "    else:\n",
    "        print(\"There is an error with artist counts!\")\n",
    "    artist_label_count_dict[artist_label_index] += 1\n",
    "        \n",
    "    # for genre labels\n",
    "    if genre_label_count_dict[genre_label_index] < 800:\n",
    "        x_tr_genre_equal.append(sample)\n",
    "        y_tr_genre_equal.append(gen_label)\n",
    "    elif 800 <= genre_label_count_dict[genre_label_index] < 900:\n",
    "        x_val_genre_equal.append(sample)\n",
    "        y_val_genre_equal.append(gen_label)\n",
    "    elif 900 <= genre_label_count_dict[genre_label_index] < 1000:\n",
    "        x_te_genre_equal.append(sample)\n",
    "        y_te_genre_equal.append(gen_label)\n",
    "    else:\n",
    "        print(\"There is an error with genre counts!\")\n",
    "    genre_label_count_dict[genre_label_index] += 1\n",
    "\n",
    "        \n",
    "        \n",
    "# turn the output datasets in np arrays\n",
    "x_tr_artist_equal = np.array(x_tr_artist_equal)\n",
    "x_val_artist_equal = np.array(x_val_artist_equal)\n",
    "x_te_artist_equal = np.array(x_te_artist_equal)\n",
    "x_tr_genre_equal = np.array(x_tr_genre_equal)\n",
    "x_val_genre_equal = np.array(x_val_genre_equal)\n",
    "x_te_genre_equal = np.array(x_te_genre_equal)\n",
    "y_tr_artist_equal = np.array(y_tr_artist_equal)\n",
    "y_val_artist_equal = np.array(y_val_artist_equal)\n",
    "y_te_artist_equal = np.array(y_te_artist_equal)\n",
    "y_tr_genre_equal = np.array(y_tr_genre_equal)\n",
    "y_val_genre_equal = np.array(y_val_genre_equal)\n",
    "y_te_genre_equal  = np.array(y_te_genre_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally prepare the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model = bpemb_en.emb\n",
    "\n",
    "embedding_dim = 50 # this depends on the embedding model we've chosen\n",
    "# create a dummy embedding weights matrix to be filled later on\n",
    "embedding_weights = np.zeros((len(vocabulary) + 1, embedding_dim)) \n",
    "\n",
    "for subword, i in vocabulary.items():\n",
    "    if subword in model.vocab:\n",
    "        embedding_vector = model[subword]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_weights[i] = embedding_vector\n",
    "    else:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 1.02473402  0.38355899 -0.079484    0.253795    1.75872803 -0.229478\n",
      "  -0.96081603 -0.009623    0.24513    -0.30851501 -0.46463799  0.82689101\n",
      "  -0.052177    0.57460701 -0.184591   -0.302048   -0.099645    0.755027\n",
      "  -0.46508199  0.115942    0.027584    0.92594999 -0.66696501 -0.133093\n",
      "   0.95801401  0.636563   -0.432753    0.27072999 -0.056136    0.597776\n",
      "   0.081105    0.48131901 -0.13154     0.120166    0.480726    0.309266\n",
      "  -0.84599602  0.50113797 -0.022155    0.17101    -0.286807   -1.35605299\n",
      "  -0.370417    0.138603   -0.65150398  0.163113    0.67434198 -1.26193595\n",
      "  -0.187253   -0.59075302]]\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print(embedding_weights[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all important variables in pickle files to a folder specific to the character model\n",
    "def writePickleChar(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/sub_word/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "\n",
    "writePickleChar(embedding_matrix, \"embedding_weights\")\n",
    "writePickleChar(x_tr_artist_equal, \"x_tr_artist_equal\")\n",
    "writePickleChar(x_val_artist_equal, \"x_val_artist_equal\")\n",
    "writePickleChar(x_te_artist_equal, \"x_te_artist_equal\")\n",
    "writePickleChar(x_tr_genre_equal, \"x_tr_genre_equal\")\n",
    "writePickleChar(x_val_genre_equal, \"x_val_genre_equal\")\n",
    "writePickleChar(x_te_genre_equal, \"x_te_genre_equal\")\n",
    "writePickleChar(y_tr_artist_equal, \"y_tr_artist_equal\")\n",
    "writePickleChar(y_val_artist_equal, \"y_val_artist_equal\")\n",
    "writePickleChar(y_te_artist_equal, \"y_te_artist_equal\")\n",
    "writePickleChar(y_tr_genre_equal, \"y_tr_genre_equal\")\n",
    "writePickleChar(y_val_genre_equal, \"y_val_genre_equal\")\n",
    "writePickleChar(y_te_genre_equal, \"y_te_genre_equal\")\n",
    "writePickleChar(vocab_size, \"vocab_size\")\n",
    "writePickleChar(length, \"sequence_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Initial Incorrect Split Code\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# store all important variables in pickle files to a folder specific to the sub-word model\n",
    "def writePickleSW(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/sub_word/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "\n",
    "writePickleSW(embedding_weights, \"embedding_weights\")\n",
    "writePickleSW(x_tr, \"x_tr\")\n",
    "writePickleSW(x_te, \"x_te\")\n",
    "writePickleSW(x_val, \"x_val\")\n",
    "writePickleSW(y_tr_artist, \"y_tr_artist\")\n",
    "writePickleSW(y_tr_genre, \"y_tr_genre\")\n",
    "writePickleSW(y_te_artist, \"y_te_artist\")\n",
    "writePickleSW(y_te_genre, \"y_te_genre\")\n",
    "writePickleSW(y_val_artist, \"y_val_artist\")\n",
    "writePickleSW(y_val_genre, \"y_val_genre\")\n",
    "vocab_size = len(vocabulary)\n",
    "writePickleSW(vocab_size, \"vocab_size\")\n",
    "writePickleSW(length, \"sequence_length\")\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(23)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_artist))) \n",
    "# shuffle all inputs with the same indices\n",
    "x_shuffled = padded_sentences[shuffle_indices]\n",
    "y_artist_shuffled = y_artist[shuffle_indices]\n",
    "y_genre_shuffled = y_genre[shuffle_indices]\n",
    "\n",
    "# Split training, validation and test sets with a ratio of 80-10-10\n",
    "training_slice = int(len(y_artist) * 0.8)\n",
    "validation_slice = int(len(y_artist) * 0.9)\n",
    "\n",
    "# training\n",
    "x_tr = x_shuffled[:training_slice]\n",
    "y_tr_artist = y_artist_shuffled[:training_slice]\n",
    "y_tr_genre = y_genre_shuffled[:training_slice]\n",
    "\n",
    "# validation\n",
    "x_val = x_shuffled[training_slice:validation_slice]\n",
    "y_val_artist = y_artist_shuffled[training_slice:validation_slice]\n",
    "y_val_genre = y_genre_shuffled[training_slice:validation_slice]\n",
    "\n",
    "# test\n",
    "x_te = x_shuffled[validation_slice:]\n",
    "y_te_artist = y_artist_shuffled[validation_slice:]\n",
    "y_te_genre = y_genre_shuffled[validation_slice:]\n",
    "\n",
    "print('Training data size is: ', x_tr.shape)\n",
    "print('Validation data size is: ', x_val.shape)\n",
    "print('Test data size is: ', x_te.shape)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
