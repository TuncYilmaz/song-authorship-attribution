{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we'll prepare our dataset for the sub-word model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by using some pickle helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def writePickle(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "def readPickle(fname):\n",
    "    filename = \"pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "def readPicklePast(fname):\n",
    "    filename = \"../pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also load the complete sub_dataset from the csv file, and get all examples with their corresponding artist labels (categorical) and genre labels (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    data = pd.read_csv('sub_dataset.csv', header=None)\n",
    "    data = data.dropna()\n",
    "\n",
    "    x = data[2]\n",
    "    x = np.array(x)\n",
    "\n",
    "    y_artist = data[0] - 1\n",
    "    y_artist = to_categorical(y_artist)\n",
    "    \n",
    "    y_genre = data[1] - 1\n",
    "    y_genre = to_categorical(y_genre)\n",
    "    \n",
    "    return (x, y_artist, y_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_artist, y_genre = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also load the genre and artist id mapping dictionaries\n",
    "artist2id = readPickle(\"artist2id\")\n",
    "id2artist = readPickle(\"id2artist\")\n",
    "genre2id = readPickle(\"genre2id\")\n",
    "id2genre = readPickle(\"id2genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spacious firmament on high,\n",
      "With all the blue ethereal sky,\n",
      "And spangled heavens, a shining frame\n",
      "Their great Original proclaim.\n",
      "The unwearied sun, from day to day,\n",
      "Does his Creator's powers display,\n",
      "And publishes to every land\n",
      "The work of an Almighty Hand.\n",
      "\n",
      "Soon as the evening shades prevail\n",
      "The moon takes up the wondrous tale,\n",
      "And nightly to the listening earth\n",
      "Repeats the story of her birth;\n",
      "While all the stars that round her burn\n",
      "And all the planets in their turn,\n",
      "Confirm the tidings as they roll,\n",
      "And spread the truth from pole to pole.\n",
      "\n",
      "What though in solemn silence all\n",
      "Move round the dark terrestrial ball?\n",
      "What though no real voice nor sound\n",
      "Amid the radiant orbs be found?\n",
      "In reason's ear they all rejoice,\n",
      "And utter forth a glorious voice,\n",
      "Forever singing as they shine,\n",
      "\"The hand that made us is divine.\"\n",
      "\n",
      "Scriptural Reference:\n",
      "\n",
      "\"The heavens declare the glory of God; the skies proclaim the work of His hands.\" Psalm 19:1 \n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Acappella \n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] Gospel&Religious\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print(x[100],'\\n', y_artist[100], id2artist[1 + np.argmax(y_artist[100], axis=-1)],'\\n', \\\n",
    "      y_genre[100], id2genre[1 + np.argmax(y_genre[100], axis=-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll try to follow the *Kim (2014)* **Convolutional Neural Networks for Sentence Classification** paper for our model. Therefore below we clean our string from a function taken from the github page of the model implementation <br> <br>\n",
    "\n",
    "For song lyrics, tabs, line breaks, punctuation like \"...\" might be very important. Therefore later we might remove/modify this preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [clean_str(sen) for sen in x]\n",
    "x = [sen.split(\" \") for sen in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'spacious', 'firmament', 'on', 'high', ',', 'with', 'all', 'the', 'blue', 'ethereal', 'sky', ',', 'and', 'spangled', 'heavens', ',', 'a', 'shining', 'frame', 'their', 'great', 'original', 'proclaim', 'the', 'unwearied', 'sun', ',', 'from', 'day', 'to', 'day', ',', 'does', 'his', 'creator', \"'s\", 'powers', 'display', ',', 'and', 'publishes', 'to', 'every', 'land', 'the', 'work', 'of', 'an', 'almighty', 'hand', 'soon', 'as', 'the', 'evening', 'shades', 'prevail', 'the', 'moon', 'takes', 'up', 'the', 'wondrous', 'tale', ',', 'and', 'nightly', 'to', 'the', 'listening', 'earth', 'repeats', 'the', 'story', 'of', 'her', 'birth', 'while', 'all', 'the', 'stars', 'that', 'round', 'her', 'burn', 'and', 'all', 'the', 'planets', 'in', 'their', 'turn', ',', 'confirm', 'the', 'tidings', 'as', 'they', 'roll', ',', 'and', 'spread', 'the', 'truth', 'from', 'pole', 'to', 'pole', 'what', 'though', 'in', 'solemn', 'silence', 'all', 'move', 'round', 'the', 'dark', 'terrestrial', 'ball', '\\\\?', 'what', 'though', 'no', 'real', 'voice', 'nor', 'sound', 'amid', 'the', 'radiant', 'orbs', 'be', 'found', '\\\\?', 'in', 'reason', \"'s\", 'ear', 'they', 'all', 'rejoice', ',', 'and', 'utter', 'forth', 'a', 'glorious', 'voice', ',', 'forever', 'singing', 'as', 'they', 'shine', ',', 'the', 'hand', 'that', 'made', 'us', 'is', 'divine', 'scriptural', 'reference', 'the', 'heavens', 'declare', 'the', 'glory', 'of', 'god', 'the', 'skies', 'proclaim', 'the', 'work', 'of', 'his', 'hands', 'psalm', '19', '1']\n"
     ]
    }
   ],
   "source": [
    "# compare the new example format with the raw version above\n",
    "print(x[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the data length properties look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1620\n",
      "3\n",
      "235.54333333333332\n"
     ]
    }
   ],
   "source": [
    "print(max([len(sentence) for sentence in x])) # max length\n",
    "print(min([len(sentence) for sentence in x])) # min length\n",
    "print(sum([len(sentence) for sentence in x])/len([len(sentence) for sentence in x])) # avg length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the maximum and the minimum length data examples are quite divergent from the average. This might result in a lot of padding for many examples. **We'll ignore this issue for now, but alternative models may reconsider cropping some data examples later on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From here on, we'll use a sub-word level embedding package retrieved from: https://github.com/bheinzerling/bpemb <br>\n",
    "#### We'll also prepare our data for keras implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sub-word level embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "# load English BPEmb model with default vocabulary size (10k) and 50-dimensional embeddings\n",
    "bpemb_en = BPEmb(lang=\"en\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a built-in function in the BPEMB library\n",
    "texts = [bpemb_en.encode(s) for s in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁i'], ['▁change'], ['▁places'], ['▁,'], ['▁to'], ['▁prevent'], ['▁catch', 'in', \"'\"], ['▁the'], ['▁cases'], ['▁races'], ['▁,'], ['▁in'], ['▁the'], ['▁faces'], ['▁,'], ['▁hall'], ['▁at'], ['▁you'], ['▁l', 'aces'], ['▁this'], ['▁is'], ['▁a'], ['▁hit'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁the'], ['▁only'], ['▁thing'], ['▁hot', 'ter'], ['▁than'], ['▁my'], ['▁flow'], ['▁is'], ['▁the'], ['▁block'], ['▁', '\\\\', '('], ['▁in', 'h', 'ale'], ['▁and'], ['▁ex', 'h', 'ale'], ['▁', '\\\\', ')'], ['▁that'], [\"▁'\", 's'], ['▁why'], ['▁i'], ['▁left'], ['▁this'], ['▁snow'], ['▁b', 'iz'], ['▁,'], ['▁and'], ['▁got'], ['▁into'], ['▁show'], ['▁b', 'iz'], ['▁let'], [\"▁'\", 's'], ['▁get'], ['▁this'], ['▁clear'], ['▁,'], ['▁it'], ['▁a', 'i'], ['▁n', \"'\", 't'], ['▁on'], ['▁till'], ['▁i'], ['▁say'], ['▁it'], [\"▁'\", 's'], ['▁on'], ['▁,'], ['▁', '\\\\', '('], ['▁p', 'ause'], ['▁', '\\\\', ')'], ['▁,'], ['▁it'], [\"▁'\", 's'], ['▁on'], ['▁i', \"'\", 'm'], ['▁eat', 'in', \"'\"], ['▁,'], ['▁y', \"'\", 'all'], ['▁n', 'ig', 'gas'], ['▁fast', 'in', \"'\"], ['▁like'], ['▁it'], [\"▁'\", 's'], ['▁rim', 'ad', 'on'], ['▁bow', 'lish'], ['▁way'], ['▁in'], ['▁leban', 'on'], ['▁,'], ['▁know'], ['▁00'], ['▁the'], ['▁bomb'], ['▁i'], ['▁be'], ['▁at'], ['▁the'], ['▁edge'], ['▁of'], ['▁the'], ['▁bar'], ['▁,'], ['▁s', 'ipp', 'in', \"'\"], ['▁a'], ['▁don'], ['▁i'], ['▁keep'], ['▁the'], ['▁bot', 'tle'], ['▁just'], ['▁in'], ['▁case'], ['▁,'], ['▁you'], ['▁never'], ['▁know'], ['▁when'], ['▁it'], [\"▁'\", 's'], ['▁on'], ['▁this'], ['▁wor', 'ries'], ['▁b', 'ump'], ['▁,'], ['▁i'], ['▁ca'], ['▁n', \"'\", 't'], ['▁go'], ['▁wrong'], ['▁,'], ['▁my'], ['▁team'], [\"▁'\", 's'], ['▁too'], ['▁strong'], ['▁you'], ['▁want'], ['▁war'], ['▁', '\\\\', '?'], ['▁i'], ['▁take'], ['▁you'], ['▁to'], ['▁war'], ['▁,'], ['▁now'], ['▁that'], ['▁my'], ['▁money'], ['▁long'], ['▁why'], ['▁you'], ['▁broke'], ['▁', '\\\\', '?'], ['▁cat'], [\"▁'\", 's'], ['▁buy'], ['▁the'], ['▁by'], ['▁lines'], ['▁and'], ['▁fant', 'as', 'ize'], ['▁the'], ['▁way'], ['▁i', \"'\", 'm'], ['▁sp', 'itt', 'in', \"'\"], ['▁,'], ['▁put'], ['▁tv'], [\"▁'\", 's'], ['▁in'], ['▁everything'], ['▁i', \"'\", 'm'], ['▁sit', 't', 'in', \"'\"], ['▁while'], ['▁i', \"'\", 'm'], ['▁hot'], ['▁to'], ['▁death'], ['▁,'], ['▁i', \"'\", 'm'], ['▁gon', 'na'], ['▁say'], ['▁this'], ['▁to'], ['▁all'], ['▁you'], ['▁play', 'a'], ['▁hat', 'ers'], ['▁y', \"'\", 'all'], ['▁should'], ['▁h', 'ate'], ['▁the'], ['▁game'], ['▁,'], ['▁not'], ['▁the'], ['▁play', 'as'], ['▁', '\\\\', '('], ['▁c', \"'\", 'mon'], ['▁', '\\\\', ')'], ['▁', '\\\\', '('], ['▁ch', 'orus'], ['▁repe', 'at'], ['▁0', 'x'], ['▁', '\\\\', ')'], ['▁i'], ['▁change'], ['▁places'], ['▁,'], ['▁to'], ['▁prevent'], ['▁catch', 'in', \"'\"], ['▁the'], ['▁cases'], ['▁races'], ['▁,'], ['▁in'], ['▁the'], ['▁faces'], ['▁,'], ['▁hall'], ['▁at'], ['▁you'], ['▁l', 'aces'], ['▁this'], ['▁is'], ['▁a'], ['▁hit'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', '('], ['▁00'], ['▁cent'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', ')'], ['▁every', 'day'], ['▁is'], ['▁bug', 'ged'], ['▁,'], ['▁n', 'ig', 'gas'], [\"▁'\", 'll'], ['▁come'], ['▁to'], ['▁a'], ['▁club'], ['▁to'], ['▁try'], ['▁to'], ['▁show'], ['▁you'], ['▁they'], ['▁a'], ['▁th', 'ug'], ['▁,'], ['▁instead'], ['▁of'], ['▁showing'], ['▁some'], ['▁love'], ['▁now'], ['▁,'], ['▁what'], ['▁you'], ['▁think'], ['▁you'], ['▁ch', 'ump'], ['▁me'], ['▁,'], ['▁if'], ['▁i'], ['▁let'], ['▁you'], ['▁b', 'ump'], ['▁me'], ['▁when'], ['▁i', \"'\", 'm'], ['▁about'], ['▁to'], ['▁make'], ['▁a'], ['▁mill'], ['▁,'], ['▁faster'], ['▁than'], ['▁you'], ['▁make'], ['▁a'], ['▁g'], ['▁', '\\\\', '('], ['▁h', 'aha'], ['▁', '\\\\', ')'], ['▁i'], ['▁know'], ['▁i'], ['▁lie'], ['▁,'], ['▁it'], [\"▁'\", 's'], ['▁a'], ['▁habit'], ['▁,'], ['▁i'], ['▁vow'], ['▁to'], ['▁clean'], ['▁the'], ['▁city'], ['▁like'], ['▁the'], ['▁mayor'], ['▁and'], ['▁in'], ['▁the'], ['▁cra', 'ck'], ['▁game'], ['▁,'], ['▁i', \"'\", 'm'], ['▁a'], ['▁franchise'], ['▁player'], ['▁n', 'ig', 'gas'], ['▁be'], ['▁think', 'in', \"'\"], ['▁i'], ['▁be'], ['▁out'], ['▁to'], ['▁lun', 'ch'], ['▁with'], ['▁mines'], ['▁then'], ['▁in'], ['▁cr', 'un', 'ch'], ['▁time'], ['▁,'], ['▁i'], ['▁start'], ['▁hit', 't', 'in', \"'\"], [\"▁'\", 'em'], ['▁hard'], ['▁with'], ['▁pun', 'ch'], ['▁lines'], ['▁you'], ['▁c', 'ats'], ['▁got'], ['▁to'], ['▁be'], ['▁sick'], ['▁,'], ['▁to'], ['▁think'], ['▁00'], ['▁ca'], ['▁n', \"'\", 't'], ['▁sp', 'it'], ['▁better'], ['▁check'], ['▁my'], ['▁batt', 'ing'], ['▁average'], ['▁,'], ['▁i'], ['▁always'], ['▁make'], ['▁hits'], ['▁my'], ['▁flows'], ['▁leave'], ['▁these'], ['▁rap'], ['▁c', 'ats'], ['▁k', 'et', 'ro'], ['▁', '\\\\', '('], ['▁k', 'et', 'ro'], ['▁', '\\\\', ')'], ['▁,'], ['▁all'], ['▁across'], ['▁the'], ['▁metro'], ['▁', '\\\\', '('], ['▁metro'], ['▁', '\\\\', ')'], ['▁plus'], ['▁i'], ['▁pack'], ['▁a'], ['▁cannon'], ['▁,'], ['▁up'], ['▁under'], ['▁my'], ['▁mar', 'ple'], ['▁cannon'], ['▁they'], ['▁f', 'ake'], ['▁,'], ['▁they'], ['▁look'], ['▁like'], ['▁money'], ['▁,'], ['▁but'], ['▁a', 'i'], ['▁n', \"'\", 't'], ['▁worth'], ['▁half'], ['▁the'], ['▁c', 'ake'], ['▁have'], ['▁me'], ['▁run', 'n', 'in', \"'\"], ['▁from'], ['▁j', 'ake'], ['▁,'], ['▁in'], ['▁a'], ['▁g', 's'], ['▁with'], ['▁bad'], ['▁bra', 'kes'], ['▁they'], ['▁want'], ['▁to'], ['▁knock'], ['▁me'], ['▁take'], ['▁,'], ['▁for'], ['▁christ'], ['▁s', 'akes'], ['▁', '\\\\', '('], ['▁00'], ['▁cent'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', ')'], ['▁i'], ['▁change'], ['▁places'], ['▁,'], ['▁to'], ['▁prevent'], ['▁catch', 'in', \"'\"], ['▁the'], ['▁cases'], ['▁races'], ['▁,'], ['▁in'], ['▁the'], ['▁faces'], ['▁,'], ['▁hall'], ['▁at'], ['▁you'], ['▁l', 'aces'], ['▁this'], ['▁is'], ['▁a'], ['▁hit'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', '('], ['▁00'], ['▁cent'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', ')'], ['▁i'], ['▁change'], ['▁places'], ['▁,'], ['▁to'], ['▁prevent'], ['▁catch', 'in', \"'\"], ['▁the'], ['▁cases'], ['▁races'], ['▁,'], ['▁in'], ['▁the'], ['▁faces'], ['▁,'], ['▁hall'], ['▁at'], ['▁you'], ['▁l', 'aces'], ['▁this'], ['▁is'], ['▁a'], ['▁hit'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', '('], ['▁00'], ['▁cent'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', ')'], ['▁y', 'o'], ['▁,'], ['▁son'], ['▁remember'], ['▁them'], ['▁f', 'ake'], ['▁play', 'as'], ['▁who'], ['▁try'], ['▁to'], ['▁play'], ['▁us'], ['▁at'], ['▁the'], ['▁sh', 'ark'], ['▁club'], ['▁in'], ['▁vegas'], ['▁had'], ['▁them'], ['▁tight'], ['▁lin', 'en'], ['▁bl', 'az', 'ers'], ['▁,'], ['▁and'], ['▁beat'], ['▁up'], ['▁g', 'ators'], ['▁look', 'in', \"'\"], ['▁like'], ['▁last'], ['▁year'], [\"▁'\", 's'], ['▁play', 'as'], ['▁,'], ['▁', '\\\\', '('], ['▁p', 'ause'], ['▁', '\\\\', ')'], ['▁ye', 'ah'], ['▁,'], ['▁i'], ['▁could'], ['▁tell'], ['▁they'], ['▁d', 'ough'], ['▁was'], ['▁low'], ['▁when'], ['▁we'], ['▁came'], ['▁through'], ['▁the'], ['▁do', \"'\"], ['▁i'], ['▁cop', 'ped'], ['▁a'], ['▁case'], ['▁of'], ['▁cr', 'ist', 'al'], ['▁,'], ['▁and'], ['▁cop', 'ped'], ['▁one'], ['▁bot', 'tle'], ['▁of'], ['▁mo'], ['▁from'], ['▁the'], ['▁looking'], ['▁through'], ['▁face'], ['▁,'], ['▁and'], ['▁the'], ['▁bul', 'ge'], ['▁in'], ['▁his'], ['▁wa', 'ist'], ['▁,'], ['▁he'], ['▁hold', 'in', \"'\"], ['▁', '\\\\', '('], ['▁ye', 'ah'], ['▁he'], [\"▁'\", 's'], ['▁pack', 'in', \"'\"], ['▁,'], ['▁i'], ['▁can'], ['▁see'], ['▁his'], ['▁ra', 'ck'], ['▁the'], ['▁one'], ['▁in'], ['▁the'], ['▁middle'], ['▁,'], ['▁he'], ['▁a'], ['▁big'], ['▁man'], ['▁,'], ['▁i'], ['▁deal', 't'], ['▁with'], ['▁him'], ['▁son'], ['▁', '\\\\', ')'], ['▁ye', 'ah'], ['▁,'], ['▁so'], ['▁i'], ['▁expect'], ['▁look'], ['▁like'], ['▁they'], ['▁a', 'i'], ['▁n', \"'\", 't'], ['▁had'], ['▁a'], ['▁run'], ['▁,'], ['▁since'], [\"▁'\"], ['▁00'], ['▁they'], ['▁a', 'i'], ['▁n', \"'\", 't'], ['▁here'], ['▁on'], ['▁a'], ['▁hunt'], ['▁for'], ['▁food'], ['▁,'], ['▁so'], ['▁they'], ['▁could'], ['▁catch'], ['▁you'], ['▁,'], ['▁some'], ['▁cash'], ['▁,'], ['▁and'], ['▁expensive'], ['▁jew', 'els'], ['▁i', \"'\", 'm'], ['▁gon', 'na'], ['▁crash'], [\"▁'\", 'em'], ['▁with'], ['▁this'], ['▁bot', 'tle'], ['▁if'], ['▁he'], ['▁move'], ['▁i'], ['▁a', 'i'], ['▁n', \"'\", 't'], ['▁the'], ['▁one'], ['▁son'], ['▁,'], ['▁my'], ['▁sh', 'it'], ['▁a', 'i'], ['▁n', \"'\", 't'], ['▁come'], ['▁easy'], ['▁it'], ['▁w', 'o'], ['▁n', \"'\", 't'], ['▁go'], ['▁easy'], ['▁,'], ['▁believe'], ['▁me'], ['▁', '\\\\', '('], ['▁ch', 'orus'], ['▁repe', 'at'], ['▁0', 'x'], ['▁', '\\\\', ')'], ['▁i'], ['▁change'], ['▁places'], ['▁,'], ['▁to'], ['▁prevent'], ['▁catch', 'in', \"'\"], ['▁the'], ['▁cases'], ['▁races'], ['▁,'], ['▁in'], ['▁the'], ['▁faces'], ['▁,'], ['▁hall'], ['▁at'], ['▁you'], ['▁l', 'aces'], ['▁this'], ['▁is'], ['▁a'], ['▁hit'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', '('], ['▁00'], ['▁cent'], ['▁,'], ['▁let'], [\"▁'\", 's'], ['▁see'], ['▁if'], ['▁hom', 'icide'], ['▁tra', 'ce'], ['▁this'], ['▁', '\\\\', ')']]\n"
     ]
    }
   ],
   "source": [
    "# show an example\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'the' is: 8 \n",
      "Index of unknown is: 4993\n",
      "The vocabulary length is: 10000\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary dictionary where each token is mapped to an index value\n",
    "vocabulary = {}\n",
    "for index, token in enumerate(bpemb_en.words):\n",
    "    vocabulary[token] = index + 1\n",
    "\n",
    "print(\"Index of 'the' is:\", vocabulary['▁the'], \"\\nIndex of unknown is:\", vocabulary[\"unk\"])\n",
    "print(\"The vocabulary length is:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writePickle(vocabulary, \"/sub_word/vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each subword to its index value with the following helper function\n",
    "def subword2index(texts, vocabulary):\n",
    "    sentences = []\n",
    "    for text in texts:\n",
    "        one_line = []\n",
    "        for sub_word in text:\n",
    "            for word in sub_word:\n",
    "                if word not in vocabulary.keys():\n",
    "                    one_line.append(vocabulary['unk'])\n",
    "                else:\n",
    "                    one_line.append(vocabulary[word])\n",
    "        sentences.append(one_line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = subword2index(texts, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[387, 2220, 3407, 1750, 43, 3178, 7819, 7, 9938, 8, 2962, 2287, 1750, 27, 8, 9424, 1750, 1500, 115, 1221, 44, 2128, 216, 81, 5, 1888, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 8, 492, 6018, 2041, 96, 539, 1242, 3229, 81, 8, 3234, 9913, 4993, 9943, 27, 9922, 566, 35, 160, 9922, 566, 9913, 4993, 9942, 121, 882, 9921, 5452, 387, 1166, 216, 4627, 21, 249, 1750, 35, 3287, 424, 702, 21, 249, 2308, 882, 9921, 2069, 216, 2647, 1750, 108, 5, 9917, 48, 9938, 9916, 72, 8707, 387, 2357, 108, 882, 9921, 72, 1750, 9913, 4993, 9943, 25, 752, 9913, 4993, 9942, 1750, 108, 882, 9921, 72, 387, 9938, 9928, 6123, 7, 9938, 1750, 156, 9938, 159, 48, 82, 5339, 3770, 7, 9938, 1069, 108, 882, 9921, 8898, 64, 10, 2538, 835, 1105, 27, 8258, 10, 1750, 2171, 40, 8, 2940, 387, 87, 115, 8, 4471, 28, 8, 736, 1750, 11, 2107, 7, 9938, 5, 1617, 387, 2902, 8, 5210, 1011, 1026, 27, 1740, 1750, 1221, 1657, 2171, 392, 108, 882, 9921, 72, 216, 206, 1345, 21, 2770, 1750, 387, 2515, 48, 9938, 9916, 823, 6896, 1750, 1242, 758, 882, 9921, 2587, 1902, 1221, 4452, 431, 9913, 4993, 9968, 387, 1618, 1221, 43, 431, 1750, 975, 121, 1242, 2796, 770, 5452, 1221, 5127, 9913, 4993, 9968, 1594, 882, 9921, 6201, 8, 102, 2675, 35, 4842, 36, 1073, 8, 1105, 387, 9938, 9928, 231, 6418, 7, 9938, 1750, 2066, 2324, 882, 9921, 27, 7082, 387, 9938, 9928, 1513, 9916, 7, 9938, 609, 387, 9938, 9928, 2041, 43, 1153, 1750, 387, 9938, 9928, 7191, 1132, 2357, 216, 43, 261, 1221, 383, 9915, 4414, 89, 156, 9938, 159, 1754, 41, 148, 8, 894, 1750, 217, 8, 383, 36, 9913, 4993, 9943, 15, 9938, 1292, 9913, 4993, 9942, 9913, 4993, 9943, 113, 6910, 3750, 18, 122, 9946, 9913, 4993, 9942, 387, 2220, 3407, 1750, 43, 3178, 7819, 7, 9938, 8, 2962, 2287, 1750, 27, 8, 9424, 1750, 1500, 115, 1221, 44, 2128, 216, 81, 5, 1888, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9943, 40, 345, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9942, 1208, 1017, 81, 8286, 962, 1750, 48, 82, 5339, 882, 1189, 2586, 43, 5, 1211, 43, 5492, 43, 702, 1221, 327, 5, 59, 398, 1750, 2104, 28, 4784, 425, 2154, 975, 1750, 1295, 1221, 3651, 1221, 113, 2770, 353, 1750, 876, 387, 2308, 1221, 21, 2770, 353, 392, 387, 9938, 9928, 593, 43, 1167, 5, 877, 1750, 8268, 539, 1221, 1167, 5, 52, 9913, 4993, 9943, 41, 9576, 9913, 4993, 9942, 387, 2171, 387, 2978, 1750, 108, 882, 9921, 5, 5761, 1750, 387, 6897, 43, 5688, 8, 471, 1069, 8, 3504, 35, 27, 8, 2739, 494, 894, 1750, 387, 9938, 9928, 5, 6535, 1479, 48, 82, 5339, 87, 3651, 7, 9938, 387, 87, 372, 43, 5891, 70, 98, 7922, 638, 27, 800, 61, 70, 440, 1750, 387, 1004, 1888, 9916, 7, 9938, 882, 95, 1921, 98, 2671, 70, 2675, 1221, 15, 1064, 3287, 43, 87, 9103, 1750, 43, 3651, 40, 2515, 48, 9938, 9916, 231, 26, 2916, 5953, 1242, 1048, 38, 1482, 1750, 387, 2966, 1167, 5351, 1242, 7263, 3911, 583, 2624, 15, 1064, 110, 66, 39, 9913, 4993, 9943, 110, 66, 39, 9913, 4993, 9942, 1750, 261, 2110, 8, 6073, 9913, 4993, 9943, 6073, 9913, 4993, 9942, 5657, 387, 3188, 5, 9393, 1750, 366, 401, 1242, 240, 870, 9393, 327, 22, 457, 1750, 327, 2341, 1069, 2796, 1750, 277, 5, 9917, 48, 9938, 9916, 5152, 1872, 8, 15, 457, 310, 353, 889, 9918, 7, 9938, 131, 100, 457, 1750, 27, 5, 52, 9921, 98, 2821, 1368, 4486, 327, 4452, 43, 7220, 353, 1618, 1750, 73, 1060, 11, 1324, 9913, 4993, 9943, 40, 345, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9942, 387, 2220, 3407, 1750, 43, 3178, 7819, 7, 9938, 8, 2962, 2287, 1750, 27, 8, 9424, 1750, 1500, 115, 1221, 44, 2128, 216, 81, 5, 1888, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9943, 40, 345, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9942, 387, 2220, 3407, 1750, 43, 3178, 7819, 7, 9938, 8, 2962, 2287, 1750, 27, 8, 9424, 1750, 1500, 115, 1221, 44, 2128, 216, 81, 5, 1888, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9943, 40, 345, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9942, 156, 9919, 1750, 1266, 6630, 620, 22, 457, 383, 36, 306, 5492, 43, 383, 196, 115, 8, 177, 926, 1211, 27, 9335, 223, 620, 9125, 3360, 20, 393, 649, 89, 1750, 35, 3452, 366, 52, 1981, 2341, 7, 9938, 1069, 1104, 297, 882, 9921, 383, 36, 1750, 9913, 4993, 9943, 25, 752, 9913, 4993, 9942, 7228, 645, 1750, 387, 1006, 3688, 327, 34, 391, 74, 1598, 392, 416, 1556, 545, 8, 550, 9938, 387, 1800, 993, 5, 1740, 28, 800, 106, 31, 1750, 35, 1800, 993, 273, 5210, 1011, 28, 1232, 131, 8, 5674, 545, 3228, 1750, 35, 8, 2653, 112, 27, 147, 3275, 106, 1750, 84, 1838, 7, 9938, 9913, 4993, 9943, 7228, 645, 84, 882, 9921, 3188, 7, 9938, 1750, 387, 281, 1843, 147, 264, 494, 8, 273, 27, 8, 2008, 1750, 84, 5, 2037, 237, 1750, 387, 2530, 9916, 98, 496, 1266, 9913, 4993, 9942, 7228, 645, 1750, 490, 387, 6683, 2341, 1069, 327, 5, 9917, 48, 9938, 9916, 223, 5, 889, 1750, 778, 882, 40, 327, 5, 9917, 48, 9938, 9916, 2836, 72, 5, 7166, 73, 2290, 1750, 490, 327, 1006, 7819, 1221, 1750, 425, 6865, 1750, 35, 7377, 1942, 891, 387, 9938, 9928, 7191, 1132, 6730, 882, 95, 98, 216, 5210, 1011, 876, 84, 2945, 387, 5, 9917, 48, 9938, 9916, 8, 273, 1266, 1750, 1242, 177, 26, 5, 9917, 48, 9938, 9916, 2586, 6471, 108, 16, 9919, 48, 9938, 9916, 823, 6471, 1750, 4753, 353, 9913, 4993, 9943, 113, 6910, 3750, 18, 122, 9946, 9913, 4993, 9942, 387, 2220, 3407, 1750, 43, 3178, 7819, 7, 9938, 8, 2962, 2287, 1750, 27, 8, 9424, 1750, 1500, 115, 1221, 44, 2128, 216, 81, 5, 1888, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9943, 40, 345, 1750, 2308, 882, 9921, 1843, 876, 2096, 5905, 405, 83, 216, 9913, 4993, 9942], [108, 5, 9917, 48, 9938, 9916, 6471, 43, 1167, 2796, 9913, 4993, 9943, 306, 9919, 9913, 4993, 9942, 490, 975, 1208, 6111, 16, 5739, 1618, 2796, 9913, 4993, 9943, 614, 9922, 5368, 9922, 1750, 614, 9922, 5368, 9922, 9913, 4993, 9942, 1221, 5, 9917, 48, 9938, 9916, 5, 59, 398, 1750, 4441, 1235, 1221, 5, 22, 457, 2796, 9913, 4993, 9943, 1618, 2796, 1750, 614, 9922, 9913, 4993, 9942, 22, 1301, 3856, 7922, 1750, 387, 9938, 1359, 1519, 1221, 115, 156, 9915, 4267, 2796, 108, 5, 9917, 48, 9938, 9916, 5, 894, 1221, 281, 613, 353, 1479, 7228, 645, 1750, 277, 387, 5, 9917, 48, 9938, 9916, 2531, 2708, 9913, 4993, 9943, 614, 9922, 5368, 9922, 9913, 4993, 9942, 4, 3802, 2357, 387, 9938, 9928, 8, 41, 1106, 125, 59, 312, 120, 7, 9938, 216, 297, 9913, 4993, 9943, 468, 9700, 1750, 2704, 2704, 2704, 9913, 4993, 9942, 27, 8, 9061, 48, 82, 5339, 2171, 1750, 510, 387, 9032, 1242, 3120, 387, 2823, 366, 2318, 1750, 35, 387, 5894, 9938, 1750, 5894, 9938, 5610, 449, 882, 95, 550, 48, 9938, 9916, 1167, 353, 889, 43, 1221, 1750, 2066, 8, 2092, 43, 1221, 310, 156, 9919, 450, 72, 1107, 1617, 645, 266, 1320, 1640, 1295, 8, 22, 1301, 387, 3271, 43, 1221, 59, 398, 48, 82, 5339, 27, 8, 1386, 4099, 387, 9938, 9928, 1960, 1354, 1221, 34, 2173, 387, 882, 1189, 588, 2694, 1221, 1208, 2033, 9929, 1301, 9, 401, 1221, 663, 2357, 113, 142, 1750, 277, 1180, 387, 550, 1750, 1295, 387, 16, 5739, 550, 73, 975, 72, 1750, 392, 387, 5448, 1750, 156, 9938, 159, 48, 82, 5339, 2916, 9141, 5452, 889, 833, 5, 5701, 391, 9933, 482, 392, 1221, 5, 9917, 48, 9938, 9916, 27, 468, 4091, 1180, 3287, 177, 26, 72, 156, 9915, 9629, 1750, 131, 261, 121, 450, 1221, 361, 8605, 38, 9913, 4993, 9943, 41, 645, 9576, 9913, 4993, 9942, 108, 5, 9917, 48, 9938, 9916, 6471, 43, 1167, 2796, 490, 975, 1208, 6111, 16, 5739, 1618, 2796, 1221, 5, 9917, 48, 9938, 9916, 5, 59, 398, 1750, 4441, 1235, 1221, 5, 22, 457, 2796, 22, 1301, 3856, 7922, 1750, 387, 9938, 1359, 1519, 1221, 115, 156, 9915, 4267, 2796, 108, 5, 9917, 48, 9938, 9916, 5, 894, 108, 5, 9917, 48, 9938, 9916, 6471, 43, 1167, 2796, 490, 975, 1208, 6111, 16, 5739, 1618, 2796, 1221, 5, 9917, 48, 9938, 9916, 5, 59, 398, 1750, 4441, 1235, 1221, 5, 22, 457, 2796, 22, 1301, 3856, 7922, 1750, 387, 9938, 1359, 1519, 1221, 115, 156, 9915, 4267, 2796, 108, 5, 9917, 48, 9938, 9916, 5, 894, 156, 9919, 572, 1242, 998, 662, 48, 82, 5339, 115, 1750, 387, 9938, 9928, 2531, 25, 76, 4818, 934, 3856, 216, 4689, 726, 1594, 1750, 21, 3014, 824, 238, 380, 5, 471, 3073, 108, 882, 9921, 48, 166, 975, 1750, 2836, 2341, 1638, 8, 1614, 975, 1242, 775, 48, 166, 1750, 35, 48, 82, 5339, 2171, 510, 387, 8465, 35, 876, 387, 550, 48, 9938, 9916, 3948, 5, 48, 82, 2260, 1750, 387, 550, 48, 9938, 9916, 2069, 72, 147, 177, 26, 156, 9938, 159, 281, 2154, 353, 1750, 128, 41, 148, 353, 1750, 128, 855, 9939, 1242, 5994, 387, 1069, 1242, 3190, 30, 1026, 1069, 2270, 1750, 468, 879, 35, 48, 82, 5339, 2142, 490, 1921, 1750, 35, 6592, 468, 450, 5452, 156, 9938, 159, 48, 82, 5339, 4521, 1069, 1750, 108, 882, 9921, 261, 1661, 27, 156, 9938, 159, 1934, 2033, 9929, 1301, 9, 1221, 5, 9917, 48, 9938, 9916, 2171, 121, 108, 882, 9921, 5, 9061, 1208, 3483, 353, 35, 40, 92, 103, 31, 1750, 468, 416, 2966, 889, 6390, 31, 5973, 48, 82, 5339, 1750, 310, 196, 1422, 38, 366, 2407, 411, 11, 1823, 372, 172, 4371, 1750, 1118, 60, 1861, 1750, 217, 1057, 3200, 30, 72, 1750, 48, 82, 5339, 1069, 5840, 397, 17, 156, 9919, 27, 327, 1895, 1750, 48, 82, 5339, 121, 550, 48, 9938, 9916, 2069, 468, 2882, 156, 9938, 159, 2902, 1242, 1533, 1750, 550, 48, 9938, 9916, 2154, 468, 3062, 2069, 5, 87, 5658, 131, 4, 9928, 1750, 35, 1026, 282, 106, 1242, 336, 9933, 490, 975, 1208, 6111, 16, 5739, 1618, 2796, 1221, 5, 9917, 48, 9938, 9916, 5, 59, 398, 1750, 4441, 1235, 1221, 5, 22, 457, 2796, 22, 1301, 3856, 7922, 1750, 387, 9938, 1359, 1519, 1221, 115, 156, 9915, 4267, 2796, 108, 5, 9917, 48, 9938, 9916, 5, 894, 108, 5, 9917, 48, 9938, 9916, 6471, 43, 1167, 2796, 490, 975, 1208, 6111, 16, 5739, 1618, 2796, 1221, 5, 9917, 48, 9938, 9916, 5, 59, 398, 1750, 4441, 1235, 1221, 5, 22, 457, 2796, 22, 1301, 3856, 7922, 1750, 387, 9938, 1359, 1519, 1221, 115, 156, 9915, 4267, 2796, 108, 5, 9917, 48, 9938, 9916, 5, 894, 156, 9919, 108, 882, 9921, 261, 593, 8, 6865, 1221, 6592, 21, 307, 347, 1221, 402, 180, 1917, 1750, 8, 104, 3402, 1221, 9555, 8, 4413, 1433, 1221, 1888, 863, 1750, 392, 1221, 1194, 8, 59, 398, 1023, 4413, 1433, 387, 550, 48, 9938, 9916, 2154, 468, 28, 882, 95, 1750, 8, 4984, 387, 9938, 9928, 3115, 882, 95, 8036, 48, 82, 5339, 387, 9938, 9928, 1960, 1354, 882, 95, 1750, 1208, 6142, 387, 2069, 237, 387, 2171, 48, 82, 5339, 81, 5, 5731, 1750, 490, 387, 6036, 261, 1242, 4941, 9930, 73, 583, 3380, 143, 22, 168, 9931, 1552, 402, 2107, 7, 9938, 1750, 3671, 122, 225, 4, 3402, 1448, 353, 43, 21, 589, 8, 21, 307, 347, 1750, 35, 8, 104, 3402, 35, 1618, 1259, 1056, 1208, 1533, 121, 2586, 372, 28, 1242, 5948, 1750, 387, 1887, 108, 1750, 1221, 1006, 6123, 882, 9925, 752, 392, 387, 8616, 1221, 1750, 1221, 7191, 9938, 15, 391, 108, 366, 1069, 1221, 2653, 78, 37, 387, 9938, 9928, 468, 1001, 2360, 1750, 277, 387, 1006, 1167, 1750, 1643, 9922, 7, 9938, 372, 1848, 217, 9922, 7, 9938, 1069, 1322, 33, 9571, 3234, 1750, 424, 5, 2739, 494, 4974, 121, 882, 9921, 25, 118, 1917, 490, 261, 1221, 48, 82, 5339, 372, 352, 1750, 8429, 1221, 8, 4630, 125, 353, 40, 1750, 387, 9938, 9928, 156, 9915, 2033, 9929, 1301, 38, 941, 1023, 5404, 662, 1452, 1750, 94, 2536, 6545, 1750, 40, 345, 1750, 48, 166, 5325, 1750, 2217, 9928, 192, 89, 1750, 316, 713, 10, 108, 5, 9917, 48, 9938, 9916, 6471, 43, 1167, 2796, 490, 975, 1208, 6111, 16, 5739, 1618, 2796, 1221, 5, 9917, 48, 9938, 9916, 5, 59, 398, 1750, 4441, 1235, 1221, 5, 22, 457, 2796, 22, 1301, 3856, 7922, 1750, 387, 9938, 1359, 1519, 1221, 115, 156, 9915, 4267, 2796, 108, 5, 9917, 48, 9938, 9916, 5, 894, 108, 5, 9917, 48, 9938, 9916, 6471, 43, 1167, 2796, 490, 975, 1208, 6111, 16, 5739, 1618, 2796, 1221, 5, 9917, 48, 9938, 9916, 5, 59, 398, 1750, 4441, 1235, 1221, 5, 22, 457, 2796, 22, 1301, 3856, 7922, 1750, 387, 9938, 1359, 1519, 1221, 115, 156, 9915, 4267, 2796, 108, 5, 9917, 48, 9938, 9916, 5, 894, 2171, 1295, 387, 9938, 9928, 4099]]\n"
     ]
    }
   ],
   "source": [
    "# show some examples\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how the sub-word level data looks like in terms of example lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  2441\n",
      "The min length is:  6\n",
      "The average length is:  304.57183333333336\n"
     ]
    }
   ],
   "source": [
    "# See sub-word level length\n",
    "length = [len(sentence) for sentence in sentences]\n",
    "print('The max length is: ', max(length))\n",
    "print('The min length is: ', min(length))\n",
    "print('The average length is: ', sum(length)/len(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap is even bigger now!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ready for the model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 2441)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded_sentences = pad_sequences(sentences, maxlen=max(length), padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "padded_sentences = np.array(padded_sentences)\n",
    "padded_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is time to split our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(23) # !!!!!! use the same seed value in all dataset split versions !!!!!!\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_artist))) \n",
    "# shuffle all inputs with the same indices\n",
    "x_shuffled = padded_sentences[shuffle_indices]\n",
    "y_artist_shuffled = y_artist[shuffle_indices]\n",
    "y_genre_shuffled = y_genre[shuffle_indices]\n",
    "\n",
    "# form count dictionaries for both artist and genre labels\n",
    "# first, artist labels\n",
    "artist_label_count_dict = dict()\n",
    "for i in range(120):\n",
    "    artist_label_count_dict[i] = 0\n",
    "# then also for genre labels\n",
    "genre_label_count_dict = dict()\n",
    "for i in range(12):\n",
    "    genre_label_count_dict[i] = 0\n",
    "    \n",
    "'''Now, we'll go through the data examples one by one. If each artist label occurs less than 80 times, the sample \n",
    "will belong to the training set; occurances between 81th and 90th times will go to the validation set and the \n",
    "occurances between 91th and 100th times (last 10 occurances) will go to the test set.\n",
    "For genre labels, any genre label occuring less than 800 times will go to the training set; occurances between 801th and \n",
    "900th times will go to the validation set and the occurances between 901th and 1000th times (last 100 occurances) will \n",
    "go to the test set.'''\n",
    "\n",
    "# create training, validation and test sets with equal distributions of artists and genres\n",
    "x_tr_artist, x_val_artist, x_te_artist = list(), list(), list()\n",
    "x_tr_genre, x_val_genre, x_te_genre = list(), list(), list()\n",
    "y_tr_artist, y_val_artist, y_te_artist = list(), list(), list()\n",
    "y_tr_genre, y_val_genre, y_te_genre = list(), list(), list()\n",
    "\n",
    "for sample, art_label, gen_label in zip(x_shuffled, y_artist_shuffled, y_genre_shuffled):\n",
    "    artist_label_index = np.argmax(art_label, axis=-1)\n",
    "    genre_label_index = np.argmax(gen_label, axis=-1)\n",
    "    # for artist labels\n",
    "    if artist_label_count_dict[artist_label_index] < 80:\n",
    "        x_tr_artist.append(sample)\n",
    "        y_tr_artist.append(art_label)\n",
    "    elif 80 <= artist_label_count_dict[artist_label_index] < 90:\n",
    "        x_val_artist.append(sample)\n",
    "        y_val_artist.append(art_label)\n",
    "    elif 90 <= artist_label_count_dict[artist_label_index] < 100:\n",
    "        x_te_artist.append(sample)\n",
    "        y_te_artist.append(art_label)\n",
    "    else:\n",
    "        print(\"There is an error with artist counts!\")\n",
    "    artist_label_count_dict[artist_label_index] += 1\n",
    "        \n",
    "    # for genre labels\n",
    "    if genre_label_count_dict[genre_label_index] < 800:\n",
    "        x_tr_genre.append(sample)\n",
    "        y_tr_genre.append(gen_label)\n",
    "    elif 800 <= genre_label_count_dict[genre_label_index] < 900:\n",
    "        x_val_genre.append(sample)\n",
    "        y_val_genre.append(gen_label)\n",
    "    elif 900 <= genre_label_count_dict[genre_label_index] < 1000:\n",
    "        x_te_genre.append(sample)\n",
    "        y_te_genre.append(gen_label)\n",
    "    else:\n",
    "        print(\"There is an error with genre counts!\")\n",
    "    genre_label_count_dict[genre_label_index] += 1\n",
    "\n",
    "        \n",
    "        \n",
    "# turn the output datasets in np arrays\n",
    "x_tr_artist = np.array(x_tr_artist)\n",
    "x_val_artist = np.array(x_val_artist)\n",
    "x_te_artist = np.array(x_te_artist)\n",
    "x_tr_genre = np.array(x_tr_genre)\n",
    "x_val_genre = np.array(x_val_genre)\n",
    "x_te_genre = np.array(x_te_genre)\n",
    "y_tr_artist = np.array(y_tr_artist)\n",
    "y_val_artist = np.array(y_val_artist)\n",
    "y_te_artist = np.array(y_te_artist)\n",
    "y_tr_genre = np.array(y_tr_genre)\n",
    "y_val_genre = np.array(y_val_genre)\n",
    "y_te_genre  = np.array(y_te_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally prepare the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model = bpemb_en.emb\n",
    "\n",
    "embedding_dim = 50 # this depends on the embedding model we've chosen\n",
    "# create a dummy embedding weights matrix to be filled later on\n",
    "embedding_weights = np.zeros((len(vocabulary) + 1, embedding_dim)) \n",
    "\n",
    "for subword, i in vocabulary.items():\n",
    "    if subword in model.vocab:\n",
    "        embedding_vector = model[subword]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_weights[i] = embedding_vector\n",
    "    else:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 1.02473402  0.38355899 -0.079484    0.253795    1.75872803 -0.229478\n",
      "  -0.96081603 -0.009623    0.24513    -0.30851501 -0.46463799  0.82689101\n",
      "  -0.052177    0.57460701 -0.184591   -0.302048   -0.099645    0.755027\n",
      "  -0.46508199  0.115942    0.027584    0.92594999 -0.66696501 -0.133093\n",
      "   0.95801401  0.636563   -0.432753    0.27072999 -0.056136    0.597776\n",
      "   0.081105    0.48131901 -0.13154     0.120166    0.480726    0.309266\n",
      "  -0.84599602  0.50113797 -0.022155    0.17101    -0.286807   -1.35605299\n",
      "  -0.370417    0.138603   -0.65150398  0.163113    0.67434198 -1.26193595\n",
      "  -0.187253   -0.59075302]]\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print(embedding_weights[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all important variables in pickle files to a folder specific to the character model\n",
    "def writePickleChar(Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/sub_word/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "\n",
    "writePickleChar(embedding_weights, \"embedding_weights\")\n",
    "writePickleChar(x_tr_artist, \"x_tr_artist\")\n",
    "writePickleChar(x_val_artist, \"x_val_artist\")\n",
    "writePickleChar(x_te_artist, \"x_te_artist\")\n",
    "writePickleChar(x_tr_genre, \"x_tr_genre\")\n",
    "writePickleChar(x_val_genre, \"x_val_genre\")\n",
    "writePickleChar(x_te_genre, \"x_te_genre\")\n",
    "writePickleChar(y_tr_artist, \"y_tr_artist\")\n",
    "writePickleChar(y_val_artist, \"y_val_artist\")\n",
    "writePickleChar(y_te_artist, \"y_te_artist\")\n",
    "writePickleChar(y_tr_genre, \"y_tr_genre\")\n",
    "writePickleChar(y_val_genre, \"y_val_genre\")\n",
    "writePickleChar(y_te_genre, \"y_te_genre\")\n",
    "writePickleChar(len(vocabulary), \"vocab_size\")\n",
    "writePickleChar(length, \"sequence_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te_genre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
